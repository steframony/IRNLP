{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caratteristiche del notebook\n",
    "\n",
    "- System_prompt presente = No\n",
    "- Dataset usato = Logic Inference OA\n",
    "- Epoche = 3\n",
    "- Fine Tuning eseguito = Si - Repo -> francescoocurcio/new_llama3.2-3B-log-ftn-lioa-3epoch_10k-sysprompt_no\n",
    "- Addestrato solo sulle risposte = No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione delle librerie e definizione delle costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:04:34.090857Z",
     "iopub.status.busy": "2025-05-10T13:04:34.090612Z",
     "iopub.status.idle": "2025-05-10T13:07:43.836996Z",
     "shell.execute_reply": "2025-05-10T13:07:43.835986Z",
     "shell.execute_reply.started": "2025-05-10T13:04:34.090836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-10T13:19:05.083724Z",
     "iopub.status.idle": "2025-05-10T13:19:05.083979Z",
     "shell.execute_reply": "2025-05-10T13:19:05.083873Z",
     "shell.execute_reply.started": "2025-05-10T13:19:05.083861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT DELLE LIBRERIE\n",
    "#########################\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Mostra tutte le colonne\n",
    "pd.set_option('display.width', None)        # Non tronca l'output a una larghezza fissa\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from unsloth import FastLanguageModel, to_sharegpt\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "#########################\n",
    "# COSTANTI\n",
    "#########################\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "OUTPUT_REPO = \"francescoocurcio/new_llama3.2-3B-log-ftn-lioa-3epoch_10k-sysprompt_no\"\n",
    "SAVE_DIRECTORY = \"/kaggle/working/new_llama3.2-3B-log-ftn-lioa-3epoch_10k-sysprompt_no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:27.416847Z",
     "iopub.status.busy": "2025-05-10T13:08:27.416296Z",
     "iopub.status.idle": "2025-05-10T13:08:28.159552Z",
     "shell.execute_reply": "2025-05-10T13:08:28.158809Z",
     "shell.execute_reply.started": "2025-05-10T13:08:27.416822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selezione e configurazione del modello: Llama3.2 3B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:08:35.512732Z",
     "iopub.status.busy": "2025-05-10T13:08:35.512001Z",
     "iopub.status.idle": "2025-05-10T13:10:13.309739Z",
     "shell.execute_reply": "2025-05-10T13:10:13.309173Z",
     "shell.execute_reply.started": "2025-05-10T13:08:35.512704Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    "    # token = \"hf...\" #Use one if using gated models like meta-llama/Llama....\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "#PEFT = Parameter Efficient Fine Tuning\n",
    "model = FastLanguageModel.get_peft_model( #Modello quantizzato\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:10:17.855616Z",
     "iopub.status.busy": "2025-05-10T13:10:17.855355Z",
     "iopub.status.idle": "2025-05-10T13:10:20.393341Z",
     "shell.execute_reply": "2025-05-10T13:10:20.392705Z",
     "shell.execute_reply.started": "2025-05-10T13:10:17.855598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = load_dataset(\"KK04/LogicInference_OA\", split=\"train\").to_pandas().sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "df = df[['INSTRUCTION','RESPONSE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:12:14.566244Z",
     "iopub.status.busy": "2025-05-10T13:12:14.565523Z",
     "iopub.status.idle": "2025-05-10T13:12:14.764953Z",
     "shell.execute_reply": "2025-05-10T13:12:14.764190Z",
     "shell.execute_reply.started": "2025-05-10T13:12:14.566217Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"Answer the following inference problem.[[:\\n{INSTRUCTION}]]\",\n",
    "    output_column_name = \"RESPONSE\",\n",
    "    conversation_extension = 1, #Select more to handle longer conversations\n",
    ")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opzione di aggiunta per il system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#system_message = {\n",
    "#    \"from\": \"system\",\n",
    "#    \"value\": (\n",
    "#        \"You are a math expert. You will be given a mathematical problem to solve. \"\n",
    "#        \"Your aim is to first provide a step-by-step explanation of the solution \"\n",
    "#        \"(integrating the formulae in LaTeX syntax) and then to conclude with a clear and concise final answer.\"\n",
    "#    )\n",
    "#}\n",
    "\n",
    "# Aggiungilo all'inizio di ogni conversazione\n",
    "#def add_system_message(example):\n",
    "#    conversation = example[\"conversations\"]\n",
    "#    return {\n",
    "#        \"conversations\": [system_message] + conversation\n",
    "#    }\n",
    "\n",
    "# Applica la funzione a tutto il dataset\n",
    "#dataset = dataset.map(add_system_message)\n",
    "#print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costruzione dataset finale conversazionale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:12:26.295264Z",
     "iopub.status.busy": "2025-05-10T13:12:26.294666Z",
     "iopub.status.idle": "2025-05-10T13:12:27.118858Z",
     "shell.execute_reply": "2025-05-10T13:12:27.117930Z",
     "shell.execute_reply.started": "2025-05-10T13:12:26.295229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:12:31.576993Z",
     "iopub.status.busy": "2025-05-10T13:12:31.576695Z",
     "iopub.status.idle": "2025-05-10T13:12:32.452015Z",
     "shell.execute_reply": "2025-05-10T13:12:32.451178Z",
     "shell.execute_reply.started": "2025-05-10T13:12:31.576968Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:13:08.494831Z",
     "iopub.status.busy": "2025-05-10T13:13:08.494538Z",
     "iopub.status.idle": "2025-05-10T13:13:08.500048Z",
     "shell.execute_reply": "2025-05-10T13:13:08.499191Z",
     "shell.execute_reply.started": "2025-05-10T13:13:08.494809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "#Anche dopo aver cambiato inizialmente queste operazioni il tokenizer a fronte delle\n",
    "#operazioni eseguite Ã¨ come se eseguisse una sorta di riformattazione\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:13:10.516119Z",
     "iopub.status.busy": "2025-05-10T13:13:10.515843Z",
     "iopub.status.idle": "2025-05-10T13:13:10.524771Z",
     "shell.execute_reply": "2025-05-10T13:13:10.524185Z",
     "shell.execute_reply.started": "2025-05-10T13:13:10.516103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = dataset[0]['text']\n",
    "tokenized = tokenizer(text, return_tensors=\"pt\", return_attention_mask=True)\n",
    "input_ids = tokenized[\"input_ids\"][0]\n",
    "\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for i, (token_id, token_str) in enumerate(zip(input_ids, decoded_tokens)):\n",
    "    print(f\"{i:03d} | Token ID: {token_id.item():>6} | Token: {repr(token_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestramento del modello tramite LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:13:20.043659Z",
     "iopub.status.busy": "2025-05-10T13:13:20.043088Z",
     "iopub.status.idle": "2025-05-10T13:13:25.063960Z",
     "shell.execute_reply": "2025-05-10T13:13:25.063076Z",
     "shell.execute_reply.started": "2025-05-10T13:13:20.043635Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            loss = logs[\"loss\"]\n",
    "            step = state.global_step\n",
    "            self.train_losses.append((step, loss))\n",
    "            print(f\"ðŸ“‰ Step {step} - Loss: {loss:.4f}\")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "loss_callback = LossLoggerCallback()\n",
    "\n",
    "#model.config.use_cache = False\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    do_train                    = True,\n",
    "\n",
    "    dataset_text_field          = \"text\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 8,\n",
    "\n",
    "    num_train_epochs            = 3,   # Epoche complete\n",
    "    #max_steps                   = 10,\n",
    "    \n",
    "    learning_rate               = 2e-4,\n",
    "    lr_scheduler_type           = \"linear\",\n",
    "    logging_strategy            = 'steps',\n",
    "    logging_steps               = 20,\n",
    "    # ðŸ’¾ Salvataggio\n",
    "    save_strategy               = 'epoch',\n",
    "    #save_steps                  = 200,\n",
    "\n",
    "    warmup_steps                = 40,\n",
    "\n",
    "    optim                       = \"adamw_8bit\",\n",
    "    seed                        = 42,\n",
    "\n",
    "    fp16                        = not is_bfloat16_supported(),\n",
    "    bf16                        = is_bfloat16_supported(),\n",
    "\n",
    "    weight_decay                = 0.01,\n",
    "    report_to                   = \"none\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    dataset_num_proc   = 2,\n",
    "    max_seq_length     = MAX_SEQ_LENGTH,\n",
    "    train_dataset      = dataset,\n",
    "    args               = training_args,\n",
    "    data_collator      = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing            = False,\n",
    "    callbacks          = [loss_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T13:13:29.868258Z",
     "iopub.status.busy": "2025-05-10T13:13:29.867927Z",
     "iopub.status.idle": "2025-05-10T13:19:05.082919Z",
     "shell.execute_reply": "2025-05-10T13:19:05.081782Z",
     "shell.execute_reply.started": "2025-05-10T13:13:29.868231Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Avvio addestramento LoRA...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvataggio del modello e visualizzazione della loss di addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:49:46.044263Z",
     "iopub.status.busy": "2025-05-10T10:49:46.043627Z",
     "iopub.status.idle": "2025-05-10T10:49:46.255531Z",
     "shell.execute_reply": "2025-05-10T10:49:46.254820Z",
     "shell.execute_reply.started": "2025-05-10T10:49:46.044239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir $SAVE_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:55:53.743230Z",
     "iopub.status.busy": "2025-05-10T10:55:53.742945Z",
     "iopub.status.idle": "2025-05-10T10:55:53.812392Z",
     "shell.execute_reply": "2025-05-10T10:55:53.811441Z",
     "shell.execute_reply.started": "2025-05-10T10:55:53.743208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PUSH MODELLO LoRA + TOKENIZER su HUGGING FACE\n",
    "print(\"ðŸ”„ Caricamento del modello e del tokenizer in corso...\")\n",
    "model.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "tokenizer.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "\n",
    "print(f\"âœ… Modello caricato correttamente su: {OUTPUT_REPO}\\n\")\n",
    "print(f\"âœ… Tokenizer caricato correttamente su: {OUTPUT_REPO}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:50:26.486446Z",
     "iopub.status.busy": "2025-05-10T10:50:26.486167Z",
     "iopub.status.idle": "2025-05-10T10:50:27.868676Z",
     "shell.execute_reply": "2025-05-10T10:50:27.867800Z",
     "shell.execute_reply.started": "2025-05-10T10:50:26.486426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SALVATAGGIO MODELLO LoRA + TOKENIZER\n",
    "print(\"ðŸ’¾ Salvataggio del modello e tokenizer...\")\n",
    "trainer.model.save_pretrained(SAVE_DIRECTORY)\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "print(f\"âœ… Modello LoRA salvato in: {SAVE_DIRECTORY}\")\n",
    "\n",
    "#Visualizzazione loss di addestramento\n",
    "def crate_loss_chart(json_file):\n",
    "    # Carica il file JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Estrai i dati di interesse\n",
    "    log_history = data.get(\"log_history\", [])\n",
    "    steps = [entry[\"step\"] for entry in log_history]\n",
    "    losses = [entry[\"loss\"] for entry in log_history]\n",
    "    \n",
    "    # Crea un DataFrame\n",
    "    df = pd.DataFrame({\"Step\": steps, \"Loss\": losses})\n",
    "    \n",
    "    # Crea il grafico a linee\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x=\"Step\", y=\"Loss\", marker='o')\n",
    "    plt.title(\"Andamento della Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # Rotazione delle etichette e selezione dei ticks\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.locator_params(axis='x', nbins=10)  # Mostra solo 10 ticks sull'asse x\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()  # Migliora la disposizione degli elementi\n",
    "    plt.show()\n",
    "\n",
    "loss_path = \"/kaggle/working/trainer_output/checkpoint-936/trainer_state.json\"\n",
    "crate_loss_chart(loss_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
