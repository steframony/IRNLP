{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import delle librerie","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nimport os\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport gc\n#import unsloth\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset, load_dataset\nfrom torch.optim import AdamW\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom tqdm import tqdm\nfrom huggingface_hub import hf_hub_download\nfrom safetensors.torch import load_file\n\npd.set_option('display.max_columns', None)  # Mostra tutte le colonne\npd.set_option('display.width', None)        # Non tronca l'output a una larghezza fissa\npd.set_option('display.max_colwidth', None)\n\nMAX_LENGHT = 2048\n\n#os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\nos.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n!huggingface-cli login --token ######","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:19:29.050007Z","iopub.execute_input":"2025-07-17T11:19:29.050273Z","iopub.status.idle":"2025-07-17T11:19:42.836222Z","shell.execute_reply.started":"2025-07-17T11:19:29.050253Z","shell.execute_reply":"2025-07-17T11:19:42.835232Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: fineGrained).\nThe token `UNIVERSAL_TOKEN` has been saved to /root/.cache/huggingface/stored_tokens\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful.\nThe current active token is: `UNIVERSAL_TOKEN`\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#File curlora.py\n\ndef compute_selection_probabilities(A):\n    column_norms_squared = torch.sum(A**2, axis=0)\n    row_norms_squared = torch.sum(A**2, axis=1)\n    total_sum_squares = torch.sum(column_norms_squared)\n    column_probs = column_norms_squared / total_sum_squares\n    row_probs = row_norms_squared / total_sum_squares\n    return column_probs, row_probs\n\n\ndef select_indices_with_replacement(probs, k):\n    inverted_P = (1 / (probs + 0.001)).float()\n\n    # Normalize the inverted probabilities\n    probs = inverted_P / inverted_P.sum()\n    \n    # Sposta su CPU e converti in numpy\n    if torch.is_tensor(probs):\n        probs = probs.detach().cpu().numpy()\n    \n    return np.random.choice(len(probs), size=k, replace=True, p=probs)\n\n\ndef adjust_duplicates(selected_indices, A, axis):\n    unique_indices, counts = np.unique(selected_indices, return_counts=True)\n    adjusted_matrix = A[:, unique_indices] if axis == 1 else A[unique_indices, :]\n    \n    for idx, count in enumerate(counts):\n        if count > 1:\n            scaling_factor = np.sqrt(count)\n            if axis == 1:\n                adjusted_matrix[:, idx] *= scaling_factor\n            else:\n                adjusted_matrix[idx, :] *= scaling_factor\n    \n    return adjusted_matrix, unique_indices\n\n\ndef cur_decomposition(A, c):\n    r = c\n    column_probs, row_probs = compute_selection_probabilities(A)\n    selected_columns = select_indices_with_replacement(column_probs, c)\n    selected_rows = select_indices_with_replacement(row_probs, r)\n    \n    C = A[:, selected_columns]\n    R = A[selected_rows, :]\n    \n    U = torch.empty(C.shape[1], R.shape[0])\n    U = torch.zeros_like(U).to(\"cuda\") #* 0.00\n    \n    return C, U, R\n\n\nclass CURModule(nn.Module):\n    def __init__(self, W, rank):\n        super(CURModule, self).__init__()\n        C, U, R = cur_decomposition(W, rank)\n        self.C = C * 1.0\n        self.R = R * 1.0\n        self.U = nn.Parameter(U)\n        #self.d = torch.nn.Dropout(0.05)\n\n    def forward(self, x):\n        W_approx = torch.matmul(torch.matmul(self.C, self.U), self.R)\n        try:\n            x = torch.matmul(x, W_approx.t())\n        except:\n            x = torch.matmul(x, W_approx)\n        #x = self.d(x)\n        return x\n\n\nclass CURLoRAMLP(nn.Module):\n    def __init__(self, base_model, rank=8, alpha=1):\n        super(CURLoRAMLP, self).__init__()\n        self.base_model = base_model\n        self.rank = rank\n        self.alpha = alpha\n        # Identify the layer to adapt (the last layer)\n        layer_to_adapt = base_model.layers[-1]\n        # Freeze the parameters of the base model\n        for param in self.base_model.parameters():\n            param.requires_grad = False\n\n        self.cur_module = CURModule(layer_to_adapt.weight, self.rank)\n    \n    def forward(self, x):\n        x = self.base_model.layers[:-1](x)  # Use all layers except the last one\n        x_0 = torch.matmul(x, self.base_model.layers[-1].weight.t()) \n        x_adapted = self.cur_module(x)\n        x = x_0 + (self.alpha * x_adapted) + self.base_model.layers[-1].bias\n        return x\n\n\nclass LinearWithCURLoRA(torch.nn.Module):\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.curlora = CURModule(linear.weight, rank)\n        self.rank = rank\n        self.alpha = alpha\n\n    def forward(self, x):\n        x_0 = self.linear(x)\n        x_adapted = self.curlora(x)\n        x = x_0 + (self.alpha * x_adapted) #+ self.linear.bias\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:19:57.310879Z","iopub.execute_input":"2025-07-17T11:19:57.311853Z","iopub.status.idle":"2025-07-17T11:19:57.326574Z","shell.execute_reply.started":"2025-07-17T11:19:57.311823Z","shell.execute_reply":"2025-07-17T11:19:57.325769Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#File utils.py\n\n# I used this function \"replace_linear_with_lora\" from\n# https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb\ndef replace_linear_with_curlora(model, rank, alpha):\n    for name, module in model.named_children():\n        #if isinstance(module, torch.nn.Linear):\n        if any(l in name for l in [\"q_proj\", \"v_proj\", \"k_proj\"]):\n            setattr(model, name, LinearWithCURLoRA(module, rank, alpha))\n        else:\n            replace_linear_with_curlora(module, rank, alpha)\n\ndef replace_linear_with_lora(model, rank, alpha):\n    for name, module in model.named_children():\n        #if isinstance(module, torch.nn.Linear):\n        if any(l in name for l in [\"q_proj\", \"v_proj\", \"k_proj\"]):\n            # Replace the Linear layer with LinearWithLoRA\n            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n        else:\n            # Recursively apply the same function to child modules\n            replace_linear_with_lora(module, rank, alpha)\n\n\ndef load_model_and_tokenizer(model_name, repo_weights_name, load_params, device = \"cuda\"):\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"cuda\"\n    )\n                                \n    tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n    tokenizer.pad_token = tokenizer.eos_token\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    #Attivazione CURLoRA\n    replace_linear_with_curlora(model, rank=8, alpha=16)\n\n    if load_params: \n        #Caricamento dei pesi del modello addestrato\n        safetensor_path = hf_hub_download(repo_id=repo_weights_name, filename=\"model.safetensors\")\n        state_dict = load_file(safetensor_path, device=device)\n        \n        model.load_state_dict(state_dict, strict=False)\n    \n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total trainable parameters after: {total_params:,}\")\n    \n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:20:00.909663Z","iopub.execute_input":"2025-07-17T11:20:00.909919Z","iopub.status.idle":"2025-07-17T11:20:00.917695Z","shell.execute_reply.started":"2025-07-17T11:20:00.909902Z","shell.execute_reply":"2025-07-17T11:20:00.916925Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Caricamento del modello","metadata":{}},{"cell_type":"code","source":"model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\nrepo_weights_name_1 = \"francescoocurcio/Llama3.2_3B_CSQA\"\nrepo_weights_name_2 = \"francescoocurcio/Llama3.2_3B_CSQA_LI\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:20:03.607101Z","iopub.execute_input":"2025-07-17T11:20:03.607364Z","iopub.status.idle":"2025-07-17T11:20:03.611122Z","shell.execute_reply.started":"2025-07-17T11:20:03.607345Z","shell.execute_reply":"2025-07-17T11:20:03.610307Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"load_model1 = False\nif load_model1:\n    model, tokenizer = load_model_and_tokenizer(model_name, repo_weights_name_1, load_params = True)\nelse:\n    model, tokenizer = load_model_and_tokenizer(model_name, repo_weights_name_2, load_params = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:20:05.265553Z","iopub.execute_input":"2025-07-17T11:20:05.266244Z","iopub.status.idle":"2025-07-17T11:22:05.777484Z","shell.execute_reply.started":"2025-07-17T11:20:05.266221Z","shell.execute_reply":"2025-07-17T11:22:05.776555Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfed709b65f74cdb9705dbf732ebb956"}},"metadata":{}},{"name":"stderr","text":"2025-07-17 11:20:08.038197: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752751208.265589      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752751208.330435      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8628ea62c45f46ca864833967a9d0186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"214d4aba8828487caf34c181ad3821af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ca3d602ad9d4b8fb3f074ce54d72a4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3cafbbf39d4c4694b03d08c6f48785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bfbb802a3c14ccc9d9f08a7bda035fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"399ef78eaf0a4d7eacd76f758b1d9175"}},"metadata":{}},{"name":"stdout","text":"Total trainable parameters after: 3,072\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"load_params = True\nif load_params:\n    model, tokenizer = load_model_and_tokenizer(model_name, repo_weights_name_2, load_params = True)\nelse:\n    model, tokenizer = load_model_and_tokenizer(model_name, repo_weights_name_2, load_params = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T09:53:42.684822Z","iopub.execute_input":"2025-07-17T09:53:42.685408Z","iopub.status.idle":"2025-07-17T09:55:50.703226Z","shell.execute_reply.started":"2025-07-17T09:53:42.685385Z","shell.execute_reply":"2025-07-17T09:55:50.702557Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af94ca2e145641a8a719d6fbaaad3be6"}},"metadata":{}},{"name":"stderr","text":"2025-07-17 09:53:45.484960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752746025.717427      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752746025.789384      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4750dedb1ec4e3abf1ba31135366aba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac55e71a319455f959a0b95a51c104e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2581e0295a8942ed833d23c0e79ae172"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2125961662f415faacb2607e0581524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b159c21e27cd4710acbe9ffc4964a788"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97a5fa0236a1413a887f29698b1f0085"}},"metadata":{}},{"name":"stdout","text":"Total trainable parameters after: 3,072\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Testing CommonSense_QA","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n# carica il dataset\ncsqa_val = load_dataset(\"tau/commonsense_qa\",split=\"validation\")\n\ndef score_text(model, tokenizer, text, device, max_length=512, do_debug=False):\n    \"\"\"\n    Calcola la loss (sum of negative log-likelihood) su tutta la sequenza `text`.\n    Restituisce un punteggio tale che la risposta più “plausibile”\n    è quella col punteggio più basso.\n    \"\"\"\n    # Tokenizzazione\n    enc = tokenizer(text,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=max_length).to(device)\n    input_ids = enc.input_ids\n    # DEBUG: cosa ricevo in input?\n    #if do_debug: \n        #print(f\"[score_text] text: {text!r}\")\n        #print(f\"[score_text] input_ids ({input_ids.shape}): {input_ids.tolist()}\")\n\n    with torch.no_grad():\n        outputs = model(input_ids, labels=input_ids)\n        # HuggingFace ritorna `loss` come media sui token validi:\n        nll = outputs.loss * input_ids.size(1)\n\n    # DEBUG: che loss ottengo?\n    #print(f\"[score_text] loss (mean): {outputs.loss.item():.4f},  nll (sum): {nll.item():.4f}\")\n    return nll.item()\n\n\ndef evaluate_csqa(model, tokenizer, val_dataset, device=\"cuda\", max_length=512, debug_examples=5):\n    \"\"\"\n    Valuta l'accuracy su CommonSenseQA mediante ranking delle opzioni\n    secondo la minore somma di NLL (o minore perplexity),\n    confrontando le lettere (A, B, C, D, E) invece degli indici.\n    \"\"\"\n    \n    model.eval()\n    model.to(device)\n\n    correct = 0\n    total = 0\n\n    for i, ex in enumerate(tqdm(val_dataset, desc=\"CSQA eval\")):\n        question = ex[\"question\"]\n        choices  = ex[\"choices\"][\"text\"]    # lista di stringhe delle opzioni\n        true_key = ex[\"answerKey\"]          # es. \"A\", \"B\", ...\n\n        do_debug = (i < debug_examples)\n        \n        #if do_debug:\n            #print(f\"\\n--- Example {i+1} ---\")\n            #print(f\"Q: {question!r}\")\n            #print(f\"Options: {choices}, true answer letter: {true_key}\")\n\n        # Calcolo dei punteggi NLL per ciascuna opzione\n        scores = []\n        for idx, choice in enumerate(choices):\n            prompt = f\"{question}  Answer: {choice}\"\n            score = score_text(model, tokenizer, prompt, device, max_length, do_debug)\n            scores.append(score)\n            # Stampo sia l'indice che la lettera corrispondente\n            letter = chr(ord('A') + idx)\n            #print(f\"  choice {letter} ({choice!r}): score = {score:.4f}\")\n\n        # Trovo l'indice della score minima e ne ricavo la lettera\n        pred_idx   = int(np.argmin(scores))\n        pred_letter = chr(ord('A') + pred_idx)\n\n        is_correct = (pred_letter == true_key)\n        #print(f\"[evaluate_csqa] Predicted letter = {pred_letter}, True letter = {true_key}, Correct? {is_correct}\")\n\n        if is_correct:\n            correct += 1\n        total += 1\n\n        #print(\"----------------------------\\n\")\n\n    accuracy = correct / total\n    print(f\"\\nFinal accuracy: {accuracy:.4f} ({correct}/{total})\")\n    return accuracy\n\nacc = evaluate_csqa(model, tokenizer, csqa_val, device=\"cuda\", max_length=512)\nprint(f\"CommonSenseQA accuracy: {acc:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:22:29.960552Z","iopub.execute_input":"2025-07-17T11:22:29.961147Z","iopub.status.idle":"2025-07-17T11:27:07.329863Z","shell.execute_reply.started":"2025-07-17T11:22:29.961124Z","shell.execute_reply":"2025-07-17T11:27:07.329056Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99f666f25554132a2faeaebbafcc7ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"886550a021a0468cb3a6a895b4f12ee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/160k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f902617fdf6245319566a3983f2dcc6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/151k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10b7a9f6e5554b6c8fe668cf57063adf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8397e367bd54329976c6a019941bcf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e258768cea4121a10a4c01ebae168f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cdfcffb04bf49368615f5713b8e0dbe"}},"metadata":{}},{"name":"stderr","text":"CSQA eval: 100%|██████████| 1221/1221 [04:34<00:00,  4.45it/s]","output_type":"stream"},{"name":"stdout","text":"\nFinal accuracy: 0.3030 (370/1221)\nCommonSenseQA accuracy: 0.30\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Verifica delle loss di addestramento","metadata":{}},{"cell_type":"code","source":"# Carica il dataset direttamente dalla tua repo\ndataset = load_dataset(\"francescoocurcio/epoch_losses_dataset_CSQA_LI\", split=\"train\")\n\n# Converti in DataFrame pandas\ndf = dataset.to_pandas()\n\n# Mostra le prime righe\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:08:38.145166Z","iopub.execute_input":"2025-05-15T12:08:38.145401Z","iopub.status.idle":"2025-05-15T12:08:40.583960Z","shell.execute_reply.started":"2025-05-15T12:08:38.145375Z","shell.execute_reply":"2025-05-15T12:08:40.583266Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/297 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4114d366cb448158ae7912517bd1847"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c3fe1f0f4fb4eeaadbc4ace48f9322b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4639ff0b1da847bcba00e97f62069bd6"}},"metadata":{}},{"name":"stdout","text":"   epoch  mean_loss\n0      1   0.349385\n1      2   0.253107\n2      3   0.238424\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dataset = load_dataset(\"francescoocurcio/all_batch_losses_dataset_CSQA_LI\", split=\"train\")\n\n# Converti in DataFrame pandas\ndf = dataset.to_pandas()\n\n# Mostra le prime righe\nprint(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T12:08:40.584794Z","iopub.execute_input":"2025-05-15T12:08:40.585055Z","iopub.status.idle":"2025-05-15T12:08:42.758366Z","shell.execute_reply.started":"2025-05-15T12:08:40.585037Z","shell.execute_reply":"2025-05-15T12:08:42.757774Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/279 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e853154864144c99a85843ec1b185692"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/112k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68f72542d5df48089164035ee83f81f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54aea0b1108149a1a153fa5108ca808c"}},"metadata":{}},{"name":"stdout","text":"       batch_loss\n0        1.516834\n1        1.360186\n2        0.803444\n3        1.758728\n4        1.563305\n...           ...\n14995    0.273831\n14996    0.488716\n14997    0.741090\n14998    0.369727\n14999    0.314238\n\n[15000 rows x 1 columns]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Calcolo della perplexity","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom tqdm import tqdm\n\n\nfrom datasets import load_dataset\n\nwikidataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n\ntxt = wikidataset[\"text\"]\ntxt = [s for s in txt if s != '']\ntxt = \"\".join(txt)\n\n\ndef calculate_perplexity(model, tokenizer, text, device='cuda', max_length=512, stride=256):\n    \"\"\"\n    Calcola la perplexity su un testo utilizzando il modello causal LM.\n    \n    Args:\n        model: AutoModelForCausalLM già in eval() mode e su device.\n        tokenizer: AutoTokenizer corrispondente.\n        text: stringa di input.\n        device: 'cuda' o 'cpu'.\n        max_length: lunghezza massima del contesto (tipicamente <= modello.config.n_positions).\n        stride: quantità di token di overlap tra finestra e finestra.\n        \n    Returns:\n        perplexity (float)\n    \"\"\"\n    model.eval()\n    model.to(device)\n    encodings = tokenizer(text, return_tensors='pt')\n    input_ids = encodings.input_ids.to(device)\n    n_tokens = input_ids.size(1)\n    nlls = []\n    # scorri il testo in finestre sovrapposte\n    for begin in tqdm(range(0, n_tokens, stride), desc=\"Calcolo Perplexity\"):\n        end = min(begin + max_length, n_tokens)\n        input_ids_slice = input_ids[:, begin:end]\n        \n        # i token di cui calcolare la loss; il resto viene ignorato\n        target_ids = input_ids_slice.clone()\n        target_ids[:, :- (end - begin - stride if end < n_tokens else 0)] = -100\n        \n        with torch.no_grad():\n            outputs = model(input_ids_slice)\n            logits = outputs.logits  # shape [1, seq_len, vocab_size]\n        \n        shift_logits = logits[..., :-1, :].contiguous()\n        shift_labels = target_ids[..., 1:].contiguous()\n        \n        loss = F.cross_entropy(\n            shift_logits.view(-1, shift_logits.size(-1)),\n            shift_labels.view(-1),\n            ignore_index=-100,\n            reduction='sum',  # somma la loss sui token validi\n        )\n        nlls.append(loss)\n    \n    total_nll = torch.stack(nlls).sum()\n    # numero totale di token validi (escludiamo quelli marcati -100)\n    valid_tokens = (input_ids.ne(tokenizer.pad_token_id)).sum() - 1  \n    perplexity = torch.exp(total_nll / valid_tokens)\n    return perplexity.item()\n\nmodel.to(\"cuda\")\nppl = calculate_perplexity(model, tokenizer, txt)\nprint(\"Perplexity:\", round(ppl, 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T11:28:02.555417Z","iopub.execute_input":"2025-07-17T11:28:02.556038Z","iopub.status.idle":"2025-07-17T11:35:42.747159Z","shell.execute_reply.started":"2025-07-17T11:28:02.556011Z","shell.execute_reply":"2025-07-17T11:35:42.746489Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44ed7856630b4a0884e4b315111dddc9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3ed31aee83f4a0489acd39ff2d253c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae6a601bca564c469615016b03396a5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc668fec45ae4d34aeb69a5a75984950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73b1a83a9274675841535fea18ca7c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"891cc456d9a84528b187a5e0b698db7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f597da7cd1f43acb42126bb146ae6b8"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (288937 > 131072). Running this sequence through the model will result in indexing errors\nCalcolo Perplexity: 100%|██████████| 1129/1129 [07:33<00:00,  2.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Perplexity: 66.6\n","output_type":"stream"}],"execution_count":7}]}