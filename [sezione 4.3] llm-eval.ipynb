{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7176fbdc",
   "metadata": {},
   "source": [
    "# LLM - EVAL\n",
    "In questo notebook si utilizza lm-eval-harness, un framework open-source sviluppato da EleutherAI, per valutare le prestazioni di modelli linguistici di grandi dimensioni (LLM) su benchmark standardizzati.\n",
    "Il tool fornisce un'interfaccia unificata per testare diversi modelli su una varietà di task linguistici, tra cui completamento di frasi, QA, classificazione, ragionamento e altro ancora.\n",
    "\n",
    "L’obiettivo è ottenere metriche quantitative su 3 dataset ben definiti, al fine di confrontare modelli diversi o monitorare l’evoluzione delle performance durante il training.\n",
    "\n",
    "In particolare i modelli coinvolti sono 3:\n",
    "1. Llama3.2-3B-it\n",
    "2. Gemma-2-2b-it\n",
    "3. Qwen3-4b-it\n",
    "\n",
    "Ed i task sulla quale verranno valutati saranno i seguenti:\n",
    "1. GSM8K per valutare le performance nell'ambito matematico;\n",
    "2. Logi_Qa 2.0 per valutare le performance nell'ambito di inferenza logica;\n",
    "3. CoQa per valutare le performance nell'ambito del senso comune;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d8264",
   "metadata": {},
   "source": [
    "## Downloading framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36de9e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T14:00:52.383847Z",
     "iopub.status.busy": "2025-07-11T14:00:52.383640Z",
     "iopub.status.idle": "2025-07-11T14:03:26.792915Z",
     "shell.execute_reply": "2025-07-11T14:03:26.791931Z"
    },
    "papermill": {
     "duration": 154.413281,
     "end_time": "2025-07-11T14:03:26.794783",
     "exception": false,
     "start_time": "2025-07-11T14:00:52.381502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/EleutherAI/lm-evaluation-harness.git\r\n",
      "  Cloning https://github.com/EleutherAI/lm-evaluation-harness.git to /tmp/pip-req-build-tyby1a70\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/EleutherAI/lm-evaluation-harness.git /tmp/pip-req-build-tyby1a70\r\n",
      "  Resolved https://github.com/EleutherAI/lm-evaluation-harness.git to commit fcddf195ec6bb69c63e36d54d75354f6ecaabab7\r\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (1.5.2)\r\n",
      "Collecting evaluate (from lm_eval==0.4.9)\r\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\r\n",
      "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (3.6.0)\r\n",
      "Collecting jsonlines (from lm_eval==0.4.9)\r\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (2.10.2)\r\n",
      "Requirement already satisfied: peft>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (0.14.0)\r\n",
      "Requirement already satisfied: pybind11>=2.6.2 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (2.13.6)\r\n",
      "Collecting pytablewriter (from lm_eval==0.4.9)\r\n",
      "  Downloading pytablewriter-1.2.1-py3-none-any.whl.metadata (38 kB)\r\n",
      "Collecting rouge-score>=0.0.4 (from lm_eval==0.4.9)\r\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting sacrebleu>=1.5.0 (from lm_eval==0.4.9)\r\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (1.2.2)\r\n",
      "Collecting sqlitedict (from lm_eval==0.4.9)\r\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (2.6.0+cu124)\r\n",
      "Collecting tqdm-multiprocess (from lm_eval==0.4.9)\r\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl.metadata (5.7 kB)\r\n",
      "Requirement already satisfied: transformers>=4.1 in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (4.51.3)\r\n",
      "Requirement already satisfied: zstandard in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (0.23.0)\r\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (0.3.8)\r\n",
      "Collecting word2number (from lm_eval==0.4.9)\r\n",
      "  Downloading word2number-1.1.zip (9.7 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Requirement already satisfied: more_itertools in /usr/local/lib/python3.11/dist-packages (from lm_eval==0.4.9) (10.6.0)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9) (25.0)\r\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9) (6.0.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9) (0.31.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->lm_eval==0.4.9) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9) (3.18.0)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9) (19.0.1)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9) (4.67.1)\r\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.16.0->lm_eval==0.4.9) (0.70.16)\r\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9)\r\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9) (1.4.0)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9) (3.9.1)\r\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score>=0.0.4->lm_eval==0.4.9) (1.17.0)\r\n",
      "Collecting portalocker (from sacrebleu>=1.5.0->lm_eval==0.4.9)\r\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9) (2024.11.6)\r\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9) (0.9.0)\r\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9) (0.4.6)\r\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu>=1.5.0->lm_eval==0.4.9) (5.3.1)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9) (1.15.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9) (1.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.1->lm_eval==0.4.9) (3.6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (12.4.127)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8->lm_eval==0.4.9)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8->lm_eval==0.4.9)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8->lm_eval==0.4.9)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8->lm_eval==0.4.9)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8->lm_eval==0.4.9)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8->lm_eval==0.4.9)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8->lm_eval==0.4.9)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8->lm_eval==0.4.9) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8->lm_eval==0.4.9) (1.3.0)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.1->lm_eval==0.4.9) (0.21.1)\r\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines->lm_eval==0.4.9) (25.3.0)\r\n",
      "Requirement already satisfied: setuptools>=38.3.0 in /usr/local/lib/python3.11/dist-packages (from pytablewriter->lm_eval==0.4.9) (75.2.0)\r\n",
      "Collecting DataProperty<2,>=1.1.0 (from pytablewriter->lm_eval==0.4.9)\r\n",
      "  Downloading DataProperty-1.1.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->lm_eval==0.4.9)\r\n",
      "  Downloading mbstrdecoder-1.1.4-py3-none-any.whl.metadata (4.3 kB)\r\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->lm_eval==0.4.9)\r\n",
      "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->lm_eval==0.4.9)\r\n",
      "  Downloading tabledata-1.3.4-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->lm_eval==0.4.9)\r\n",
      "  Downloading tcolorpy-0.1.7-py3-none-any.whl.metadata (6.3 kB)\r\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.9)\r\n",
      "  Downloading typepy-1.3.4-py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9) (3.11.18)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0->lm_eval==0.4.9) (1.1.0)\r\n",
      "Requirement already satisfied: chardet<6,>=3.0.4 in /usr/local/lib/python3.11/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm_eval==0.4.9) (5.2.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.16.0->lm_eval==0.4.9) (2025.4.26)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.9) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2018.9 in /usr/local/lib/python3.11/dist-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==0.4.9) (2025.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8->lm_eval==0.4.9) (3.0.2)\r\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score>=0.0.4->lm_eval==0.4.9) (8.1.8)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.16.0->lm_eval==0.4.9) (2025.2)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9) (1.3.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9) (1.6.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9) (6.4.3)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.16.0->lm_eval==0.4.9) (1.20.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0.0,>=1.17->accelerate>=0.26.0->lm_eval==0.4.9) (2024.2.0)\r\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\r\n",
      "Downloading pytablewriter-1.2.1-py3-none-any.whl (91 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\r\n",
      "Downloading DataProperty-1.1.0-py3-none-any.whl (27 kB)\r\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mbstrdecoder-1.1.4-py3-none-any.whl (7.9 kB)\r\n",
      "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\r\n",
      "Downloading tabledata-1.3.4-py3-none-any.whl (11 kB)\r\n",
      "Downloading tcolorpy-0.1.7-py3-none-any.whl (8.1 kB)\r\n",
      "Downloading typepy-1.3.4-py3-none-any.whl (31 kB)\r\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\r\n",
      "Building wheels for collected packages: lm_eval, rouge-score, sqlitedict, word2number\r\n",
      "  Building wheel for lm_eval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for lm_eval: filename=lm_eval-0.4.9-py3-none-any.whl size=7462285 sha256=75bbdaac049f121e7ce14da5becb867964ae1ddf9f578bc3e49b3a53381ae042\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ienrsnqt/wheels/14/4f/5c/a21f2eb9ccc91346023767a7fbab39d02217e68c93fc36cf7f\r\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b88d6115dd56a241e10e2ca3c03e6f152f4a8c3e570935a2f848c77e8d0cf888\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\r\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16862 sha256=e93067bc9f1a8f3667e4b2767ea813de01a5f464ec64ee6638dc1455828ffc3b\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\r\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=41fc01b7a30f05db7990a376824d2f9676c2945140d1abf71b64173d3cb646ce\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/cd/ef/ae/073b491b14d25e2efafcffca9e16b2ee6d114ec5c643ba4f06\r\n",
      "Successfully built lm_eval rouge-score sqlitedict word2number\r\n",
      "Installing collected packages: word2number, sqlitedict, tqdm-multiprocess, tcolorpy, portalocker, pathvalidate, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, mbstrdecoder, jsonlines, fsspec, typepy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, DataProperty, tabledata, pytablewriter, sacrebleu, rouge-score, evaluate, lm_eval\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.10.19\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.10.19:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.4.0.6\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.4.0.6:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.9.0.13\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.9.0.13:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2025.3.2\r\n",
      "    Uninstalling fsspec-2025.3.2:\r\n",
      "      Successfully uninstalled fsspec-2025.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed DataProperty-1.1.0 evaluate-0.4.5 fsspec-2025.3.0 jsonlines-4.0.0 lm_eval-0.4.9 mbstrdecoder-1.1.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pathvalidate-3.3.1 portalocker-3.2.0 pytablewriter-1.2.1 rouge-score-0.1.2 sacrebleu-2.5.1 sqlitedict-2.1.0 tabledata-1.3.4 tcolorpy-0.1.7 tqdm-multiprocess-0.0.11 typepy-1.3.4 word2number-1.1\r\n",
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.13.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.46.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/EleutherAI/lm-evaluation-harness.git\n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bfbc0b",
   "metadata": {},
   "source": [
    "# Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f334aa20",
   "metadata": {},
   "source": [
    "## MATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd78943",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d12521f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-30 10:37:24.350809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1748601444.538776     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1748601444.599472     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 5.12MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 4.88MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 54.8MB/s]\r\n",
      "tokenizer.json: 100%|███████████████████████| 17.5M/17.5M [00:00<00:00, 163MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 5.66MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 41.4MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|████▉| 4.99G/4.99G [00:09<00:00, 520MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|██████▉| 241M/241M [00:01<00:00, 167MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.70s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.19MB/s]\r\n",
      "README.md: 100%|███████████████████████████| 7.94k/7.94k [00:00<00:00, 41.0MB/s]\r\n",
      "train-00000-of-00001.parquet: 100%|████████| 2.31M/2.31M [00:00<00:00, 36.5MB/s]\r\n",
      "test-00000-of-00001.parquet: 100%|████████████| 419k/419k [00:00<00:00, 318MB/s]\r\n",
      "Generating train split: 100%|████| 7473/7473 [00:00<00:00, 100542.85 examples/s]\r\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 181541.21 examples/s]\r\n",
      "100%|██████████████████████████████████████| 1319/1319 [00:05<00:00, 222.26it/s]\r\n",
      "Running generate_until requests:   0%|                 | 0/1319 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|████| 1319/1319 [2:57:37<00:00,  8.08s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,load_in_4bit=True,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.4503|±  |0.0137|\r\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.0212|±  |0.0040|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,load_in_4bit=True,dtype=\"float16\" \\\n",
    "    --tasks gsm8k \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8\\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8246f714",
   "metadata": {},
   "source": [
    "### Post Fine Tuning 1\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bdb01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-30 07:18:44.806153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1748589525.012036     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1748589525.071567     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 5.46MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 3.95MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 49.1MB/s]\r\n",
      "tokenizer.json: 100%|███████████████████████| 17.5M/17.5M [00:00<00:00, 206MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 6.38MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 18.5MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|████▉| 4.99G/4.99G [00:09<00:00, 505MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|██████▉| 241M/241M [00:01<00:00, 143MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.54s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 2.13MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 800/800 [00:00<00:00, 9.09MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████| 41.6M/41.6M [00:00<00:00, 55.6MB/s]\r\n",
      "README.md: 100%|███████████████████████████| 7.94k/7.94k [00:00<00:00, 35.6MB/s]\r\n",
      "train-00000-of-00001.parquet: 100%|████████| 2.31M/2.31M [00:00<00:00, 30.4MB/s]\r\n",
      "test-00000-of-00001.parquet: 100%|████████████| 419k/419k [00:00<00:00, 268MB/s]\r\n",
      "Generating train split: 100%|████| 7473/7473 [00:00<00:00, 114453.18 examples/s]\r\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 168410.56 examples/s]\r\n",
      "100%|██████████████████████████████████████| 1319/1319 [00:05<00:00, 226.89it/s]\r\n",
      "Running generate_until requests:   0%|                 | 0/1319 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:463: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|████| 1319/1319 [3:00:59<00:00,  8.23s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,load_in_4bit=True,peft=stefra/GEMMA2BITMATHR8A16,dtype=float16), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.2570|±  |0.0120|\r\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.1061|±  |0.0085|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,load_in_4bit=True,peft=stefra/GEMMA2BITMATHR8A16,dtype=\"float16\" \\\n",
    "    --tasks gsm8k \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8\\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a431759",
   "metadata": {},
   "source": [
    "### Post Fine Tuning 2\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al secondo training effettuato, quindi con una configurazione di LoRA pari ad: (r=4 ed $\\alpha=8$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "160c4265",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T14:03:26.937801Z",
     "iopub.status.busy": "2025-07-11T14:03:26.937505Z",
     "iopub.status.idle": "2025-07-11T18:15:15.417661Z",
     "shell.execute_reply": "2025-07-11T18:15:15.416685Z"
    },
    "papermill": {
     "duration": 15108.529832,
     "end_time": "2025-07-11T18:15:15.419204",
     "exception": false,
     "start_time": "2025-07-11T14:03:26.889372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-11 14:03:43.528793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752242623.871056     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752242623.971466     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 5.06MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 4.67MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 8.05MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 35.9MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 4.85MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 39.1MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.99G/4.99G [00:28<00:00, 172MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|███████| 241M/241M [00:01<00:00, 134MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.82s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.60MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 799/799 [00:00<00:00, 5.31MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████| 20.8M/20.8M [00:00<00:00, 39.1MB/s]\r\n",
      "README.md: 7.94kB [00:00, 26.0MB/s]\r\n",
      "main/train-00000-of-00001.parquet: 100%|███| 2.31M/2.31M [00:00<00:00, 5.33MB/s]\r\n",
      "main/test-00000-of-00001.parquet: 100%|██████| 419k/419k [00:00<00:00, 1.32MB/s]\r\n",
      "Generating train split: 100%|█████| 7473/7473 [00:00<00:00, 69537.05 examples/s]\r\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 283867.15 examples/s]\r\n",
      "100%|██████████████████████████████████████| 1319/1319 [00:05<00:00, 230.31it/s]\r\n",
      "Running generate_until requests: 100%|████| 1319/1319 [4:09:58<00:00, 11.37s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,load_in_4bit=True,peft=stefra/mathprova), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.3207|±  |0.0129|\r\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.2570|±  |0.0120|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,load_in_4bit=True,peft=stefra/mathprova \\\n",
    "    --tasks gsm8k \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8\\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5e55f5",
   "metadata": {},
   "source": [
    "## Logic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd1d6d",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59fc26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-11 09:17:19.411682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752225439.774111     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752225439.880910     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 4.64MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 4.65MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 9.17MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 32.1MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 4.19MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 87.2MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.99G/4.99G [00:33<00:00, 149MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|██████| 241M/241M [00:03<00:00, 78.6MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.67s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.03MB/s]\r\n",
      "README.md: 1.50kB [00:00, 5.99MB/s]\r\n",
      "logiqa2.py: 9.97kB [00:00, 31.0MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for baber/logiqa2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/baber/logiqa2\r\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\r\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n",
      "  warnings.warn(\r\n",
      "Downloading data: 100%|████████████████████| 14.0M/14.0M [00:01<00:00, 7.20MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.74M/1.74M [00:00<00:00, 38.7MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.77M/1.77M [00:00<00:00, 67.6MB/s]\r\n",
      "Generating train split: 12567 examples [00:01, 11772.46 examples/s]\r\n",
      "Generating test split: 1572 examples [00:00, 16191.76 examples/s]\r\n",
      "Generating validation split: 1569 examples [00:00, 15152.00 examples/s]\r\n",
      "100%|█████████████████████████████████████| 1572/1572 [00:00<00:00, 2078.67it/s]\r\n",
      "Running loglikelihood requests: 100%|███████| 6288/6288 [56:19<00:00,  1.86it/s]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,load_in_4bit=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "| Tasks |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\r\n",
      "|-------|------:|------|-----:|--------|---|-----:|---|-----:|\r\n",
      "|logiqa2|      0|none  |     0|acc     |↑  |0.2678|±  |0.0112|\r\n",
      "|       |       |none  |     0|acc_norm|↑  |0.2824|±  |0.0114|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,load_in_4bit=True \\\n",
    "    --tasks logiqa2 \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cda53be",
   "metadata": {},
   "source": [
    "### Post fine-tuning 1\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aaef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-11 08:46:28.853339: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752223589.222103     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752223589.326974     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 4.00MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 4.87MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 8.43MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 40.6MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 3.94MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 73.8MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.99G/4.99G [00:28<00:00, 173MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|███████| 241M/241M [00:01<00:00, 145MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:08<00:00,  4.16s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.56MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 800/800 [00:00<00:00, 5.17MB/s]\r\n",
      "adapter_model.safetensors: 100%|████████████| 41.6M/41.6M [00:00<00:00, 162MB/s]\r\n",
      "README.md: 1.50kB [00:00, 5.22MB/s]\r\n",
      "logiqa2.py: 9.97kB [00:00, 26.7MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for baber/logiqa2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/baber/logiqa2\r\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\r\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n",
      "  warnings.warn(\r\n",
      "Downloading data: 100%|█████████████████████| 14.0M/14.0M [00:00<00:00, 123MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 1.74M/1.74M [00:00<00:00, 125MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 1.77M/1.77M [00:00<00:00, 365MB/s]\r\n",
      "Generating train split: 12567 examples [00:00, 15084.92 examples/s]\r\n",
      "Generating test split: 1572 examples [00:00, 15575.30 examples/s]\r\n",
      "Generating validation split: 1569 examples [00:00, 16235.17 examples/s]\r\n",
      "100%|█████████████████████████████████████| 1572/1572 [00:00<00:00, 2142.54it/s]\r\n",
      "Running loglikelihood requests: 100%|███████| 6288/6288 [56:39<00:00,  1.85it/s]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,load_in_4bit=True,peft=stefra/GEMMA2BITLOGICINFERENCE,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "| Tasks |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\r\n",
      "|-------|------:|------|-----:|--------|---|-----:|---|-----:|\r\n",
      "|logiqa2|      0|none  |     0|acc     |↑  |0.2780|±  |0.0113|\r\n",
      "|       |       |none  |     0|acc_norm|↑  |0.2863|±  |0.0114|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,load_in_4bit=True,peft=stefra/GEMMA2BITLOGICINFERENCE \\\n",
    "    --tasks logiqa2 \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e8213",
   "metadata": {},
   "source": [
    "### Post fine-tuning 2\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=4 ed $\\alpha=8$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1e63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 17:05:44.360165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752339944.558958     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752339944.618054     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 4.42MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 8.90MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 7.15MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 32.5MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 4.29MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 67.4MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.99G/4.99G [00:32<00:00, 152MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|██████| 241M/241M [00:02<00:00, 90.8MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.56s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.31MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 799/799 [00:00<00:00, 5.73MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████| 20.8M/20.8M [00:01<00:00, 14.3MB/s]\r\n",
      "README.md: 1.50kB [00:00, 8.77MB/s]\r\n",
      "logiqa2.py: 9.97kB [00:00, 30.6MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for baber/logiqa2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/baber/logiqa2\r\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\r\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n",
      "  warnings.warn(\r\n",
      "Downloading data: 100%|████████████████████| 14.0M/14.0M [00:00<00:00, 18.8MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.74M/1.74M [00:00<00:00, 22.1MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.77M/1.77M [00:00<00:00, 68.6MB/s]\r\n",
      "Generating train split: 12567 examples [00:00, 13725.76 examples/s]\r\n",
      "Generating test split: 1572 examples [00:00, 16600.19 examples/s]\r\n",
      "Generating validation split: 1569 examples [00:00, 16544.43 examples/s]\r\n",
      "100%|█████████████████████████████████████| 1572/1572 [00:00<00:00, 2143.95it/s]\r\n",
      "Running loglikelihood requests: 100%|███████| 6288/6288 [56:40<00:00,  1.85it/s]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,peft=stefra/logicinference,load_in_4bit=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "| Tasks |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\r\n",
      "|-------|------:|------|-----:|--------|---|-----:|---|-----:|\r\n",
      "|logiqa2|      0|none  |     0|acc     |↑  |0.2850|±  |0.0114|\r\n",
      "|       |       |none  |     0|acc_norm|↑  |0.2888|±  |0.0114|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,peft=stefra/logicinference,load_in_4bit=True\\\n",
    "    --tasks logiqa2 \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90398d7",
   "metadata": {},
   "source": [
    "## Common Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca62d7d0",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f631178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 09:34:23.254584: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752312863.486427     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752312863.550850     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 4.30MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 5.09MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 9.40MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 37.1MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 5.16MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|█████████| 24.2k/24.2k [00:00<00:00, 102MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.99G/4.99G [00:30<00:00, 163MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|███████| 241M/241M [00:01<00:00, 137MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:10<00:00,  5.41s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.09MB/s]\r\n",
      "README.md: 100%|███████████████████████████████| 985/985 [00:00<00:00, 6.80MB/s]\r\n",
      "dataset_infos.json: 3.48kB [00:00, 16.2MB/s]\r\n",
      "coqa_train.parquet: 100%|██████████████████| 15.2M/15.2M [00:00<00:00, 27.2MB/s]\r\n",
      "coqa_validation.parquet: 100%|█████████████| 2.22M/2.22M [00:00<00:00, 6.70MB/s]\r\n",
      "Generating train split: 100%|█████| 7199/7199 [00:00<00:00, 42105.40 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 24613.59 examples/s]\r\n",
      "100%|██████████████████████████████████████| 500/500 [00:00<00:00, 10776.40it/s]\r\n",
      "Running generate_until requests: 100%|██████| 500/500 [1:05:02<00:00,  7.80s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,load_in_4bit=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\r\n",
      "|-----|------:|------|-----:|------|---|-----:|---|-----:|\r\n",
      "|coqa |      3|none  |     0|em    |↑  |0.3338|±  |0.0197|\r\n",
      "|     |       |none  |     0|f1    |↑  |0.5336|±  |0.0169|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,load_in_4bit=True \\\n",
    "    --tasks coqa \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a46b8b",
   "metadata": {},
   "source": [
    "### Post fine-tuning 1\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cafff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 14:11:14.492645: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752329474.851042     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752329474.959658     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 5.98MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 4.77MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 9.49MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 40.1MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 7.00MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 81.0MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.99G/4.99G [00:33<00:00, 148MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|███████| 241M/241M [00:02<00:00, 114MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.52s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.42MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 800/800 [00:00<00:00, 5.57MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████| 41.6M/41.6M [00:00<00:00, 53.2MB/s]\r\n",
      "README.md: 100%|███████████████████████████████| 985/985 [00:00<00:00, 8.07MB/s]\r\n",
      "dataset_infos.json: 3.48kB [00:00, 17.3MB/s]\r\n",
      "coqa_train.parquet: 100%|██████████████████| 15.2M/15.2M [00:00<00:00, 24.9MB/s]\r\n",
      "coqa_validation.parquet: 100%|█████████████| 2.22M/2.22M [00:00<00:00, 6.72MB/s]\r\n",
      "Generating train split: 100%|█████| 7199/7199 [00:00<00:00, 28433.83 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 18598.20 examples/s]\r\n",
      "100%|██████████████████████████████████████| 500/500 [00:00<00:00, 12095.63it/s]\r\n",
      "Running generate_until requests: 100%|████████| 500/500 [17:24<00:00,  2.09s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,peft=stefra/GEMMA2BITCOMMONSENSE,load_in_4bit=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\r\n",
      "|-----|------:|------|-----:|------|---|-----:|---|-----:|\r\n",
      "|coqa |      3|none  |     0|em    |↑  |0.5608|±  |0.0197|\r\n",
      "|     |       |none  |     0|f1    |↑  |0.6982|±  |0.0162|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,peft=stefra/GEMMA2BITCOMMONSENSE,load_in_4bit=True\\\n",
    "    --tasks coqa \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292987ac",
   "metadata": {},
   "source": [
    "### Post fine-tuning 2\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=4 ed $\\alpha=8$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae79468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 12:13:06.383149: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752322386.563280     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752322386.611843     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 100%|█████████████████████████████| 838/838 [00:00<00:00, 7.83MB/s]\r\n",
      "tokenizer_config.json: 100%|███████████████| 47.0k/47.0k [00:00<00:00, 4.71MB/s]\r\n",
      "tokenizer.model: 100%|█████████████████████| 4.24M/4.24M [00:00<00:00, 8.26MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.5M/17.5M [00:00<00:00, 29.1MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 636/636 [00:00<00:00, 3.51MB/s]\r\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\r\n",
      "model.safetensors.index.json: 100%|████████| 24.2k/24.2k [00:00<00:00, 25.3MB/s]\r\n",
      "model-00001-of-00002.safetensors: 100%|█████| 4.99G/4.99G [00:32<00:00, 153MB/s]\r\n",
      "model-00002-of-00002.safetensors: 100%|███████| 241M/241M [00:01<00:00, 151MB/s]\r\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:07<00:00,  3.52s/it]\r\n",
      "generation_config.json: 100%|██████████████████| 187/187 [00:00<00:00, 1.54MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 799/799 [00:00<00:00, 8.04MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████| 20.8M/20.8M [00:01<00:00, 14.5MB/s]\r\n",
      "README.md: 100%|███████████████████████████████| 985/985 [00:00<00:00, 9.96MB/s]\r\n",
      "dataset_infos.json: 3.48kB [00:00, 11.0MB/s]\r\n",
      "coqa_train.parquet: 100%|██████████████████| 15.2M/15.2M [00:00<00:00, 28.0MB/s]\r\n",
      "coqa_validation.parquet: 100%|█████████████| 2.22M/2.22M [00:00<00:00, 6.46MB/s]\r\n",
      "Generating train split: 100%|█████| 7199/7199 [00:00<00:00, 43375.97 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 14493.90 examples/s]\r\n",
      "100%|██████████████████████████████████████| 500/500 [00:00<00:00, 13421.52it/s]\r\n",
      "Running generate_until requests: 100%|████████| 500/500 [21:23<00:00,  2.57s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=google/gemma-2-2b-it,peft=stefra/commonsense,load_in_4bit=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\r\n",
      "|-----|------:|------|-----:|------|---|-----:|---|-----:|\r\n",
      "|coqa |      3|none  |     0|em    |↑  |0.5522|±  |0.0195|\r\n",
      "|     |       |none  |     0|f1    |↑  |0.6821|±  |0.0168|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=google/gemma-2-2b-it,peft=stefra/commonsense,load_in_4bit=True\\\n",
    "    --tasks coqa \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./gemma \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17935e5f",
   "metadata": {},
   "source": [
    "# LLama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b04833",
   "metadata": {},
   "source": [
    "## MATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af432a61",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52614475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-11 04:44:44.060729: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752209084.439787     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752209084.542368     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.47kB [00:00, 6.96MB/s]\r\n",
      "tokenizer_config.json: 54.7kB [00:00, 118MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 28.7MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 454/454 [00:00<00:00, 3.32MB/s]\r\n",
      "chat_template.jinja: 3.83kB [00:00, 20.7MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 2.24G/2.24G [00:09<00:00, 243MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 234/234 [00:00<00:00, 1.42MB/s]\r\n",
      "README.md: 7.94kB [00:00, 26.7MB/s]\r\n",
      "main/train-00000-of-00001.parquet: 100%|███| 2.31M/2.31M [00:00<00:00, 6.10MB/s]\r\n",
      "main/test-00000-of-00001.parquet: 100%|██████| 419k/419k [00:00<00:00, 1.37MB/s]\r\n",
      "Generating train split: 100%|█████| 7473/7473 [00:00<00:00, 44622.03 examples/s]\r\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 277808.93 examples/s]\r\n",
      "100%|██████████████████████████████████████| 1319/1319 [00:06<00:00, 200.58it/s]\r\n",
      "Running generate_until requests:   0%|                 | 0/1319 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|████| 1319/1319 [4:50:01<00:00, 13.19s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,parallelize=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.6823|±  |0.0128|\r\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.2229|±  |0.0115|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,parallelize=True \\\n",
    "    --tasks gsm8k \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8\\\n",
    "    --output_path ./llama \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a04c47",
   "metadata": {},
   "source": [
    "### Post Fine-Tuning\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837e5685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-10 16:25:40.914093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752164741.128573     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752164741.188146     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.47kB [00:00, 5.70MB/s]\r\n",
      "tokenizer_config.json: 54.7kB [00:00, 104MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 28.1MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 454/454 [00:00<00:00, 2.30MB/s]\r\n",
      "chat_template.jinja: 3.83kB [00:00, 14.7MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 2.24G/2.24G [00:09<00:00, 230MB/s]\r\n",
      "generation_config.json: 100%|███████████████████| 234/234 [00:00<00:00, 155kB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 817/817 [00:00<00:00, 4.28MB/s]\r\n",
      "adapter_model.safetensors: 100%|██████████▉| 97.3M/97.3M [00:04<00:00, 23.4MB/s]\r\n",
      "README.md: 7.94kB [00:00, 24.5MB/s]\r\n",
      "main/train-00000-of-00001.parquet: 100%|███| 2.31M/2.31M [00:00<00:00, 4.19MB/s]\r\n",
      "main/test-00000-of-00001.parquet: 100%|██████| 419k/419k [00:00<00:00, 1.48MB/s]\r\n",
      "Generating train split: 100%|████| 7473/7473 [00:00<00:00, 108537.99 examples/s]\r\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 244305.01 examples/s]\r\n",
      "100%|██████████████████████████████████████| 1319/1319 [00:05<00:00, 246.80it/s]\r\n",
      "Running generate_until requests:   0%|                 | 0/1319 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|████| 1319/1319 [5:35:25<00:00, 15.26s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,peft=francescoocurcio/new_llama3.2-3B-math-ftn-math-3epoch_12.5k-sysprompt_no,parallelize=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.2843|±  |0.0124|\r\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.1774|±  |0.0105|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,peft=francescoocurcio/new_llama3.2-3B-math-ftn-math-3epoch_12.5k-sysprompt_no,parallelize=True \\\n",
    "    --tasks gsm8k \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8\\\n",
    "    --output_path ./llama \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe583b",
   "metadata": {},
   "source": [
    "## Logic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c56e3c",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69150be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 17:13:22.403528: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752340402.767140     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752340402.871753     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.47kB [00:00, 7.35MB/s]\r\n",
      "tokenizer_config.json: 54.7kB [00:00, 114MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 28.9MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 454/454 [00:00<00:00, 3.14MB/s]\r\n",
      "chat_template.jinja: 3.83kB [00:00, 19.1MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 2.24G/2.24G [00:09<00:00, 225MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 234/234 [00:00<00:00, 1.64MB/s]\r\n",
      "README.md: 1.50kB [00:00, 5.94MB/s]\r\n",
      "logiqa2.py: 9.97kB [00:00, 30.8MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for baber/logiqa2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/baber/logiqa2\r\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\r\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n",
      "  warnings.warn(\r\n",
      "Downloading data: 100%|████████████████████| 14.0M/14.0M [00:01<00:00, 7.38MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.74M/1.74M [00:00<00:00, 17.1MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.77M/1.77M [00:00<00:00, 78.8MB/s]\r\n",
      "Generating train split: 12567 examples [00:00, 12923.54 examples/s]\r\n",
      "Generating test split: 1572 examples [00:00, 8408.30 examples/s]\r\n",
      "Generating validation split: 1569 examples [00:00, 9266.99 examples/s]\r\n",
      "100%|█████████████████████████████████████| 1572/1572 [00:00<00:00, 1919.40it/s]\r\n",
      "Running loglikelihood requests: 100%|█████| 6288/6288 [1:21:56<00:00,  1.28it/s]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "| Tasks |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\r\n",
      "|-------|------:|------|-----:|--------|---|-----:|---|-----:|\r\n",
      "|logiqa2|      0|none  |     0|acc     |↑  |0.2774|±  |0.0113|\r\n",
      "|       |       |none  |     0|acc_norm|↑  |0.2952|±  |0.0115|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,parallelize=True \\\n",
    "    --tasks logiqa2 \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./llama \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b866cf",
   "metadata": {},
   "source": [
    "### Post Fine-Tuning\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ffd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 18:38:15.279387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752345495.540621     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752345495.621301     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.47kB [00:00, 7.89MB/s]\r\n",
      "tokenizer_config.json: 54.7kB [00:00, 141MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:01<00:00, 13.4MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 454/454 [00:00<00:00, 2.29MB/s]\r\n",
      "chat_template.jinja: 3.83kB [00:00, 2.50MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 2.24G/2.24G [00:08<00:00, 269MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 234/234 [00:00<00:00, 1.21MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 817/817 [00:00<00:00, 5.18MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████▉| 97.3M/97.3M [00:00<00:00, 154MB/s]\r\n",
      "README.md: 1.50kB [00:00, 7.93MB/s]\r\n",
      "logiqa2.py: 9.97kB [00:00, 31.8MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for baber/logiqa2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/baber/logiqa2\r\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\r\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n",
      "  warnings.warn(\r\n",
      "Downloading data: 100%|████████████████████| 14.0M/14.0M [00:00<00:00, 14.8MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.74M/1.74M [00:00<00:00, 88.4MB/s]\r\n",
      "Downloading data: 100%|█████████████████████| 1.77M/1.77M [00:00<00:00, 357MB/s]\r\n",
      "Generating train split: 12567 examples [00:00, 15725.72 examples/s]\r\n",
      "Generating test split: 1572 examples [00:00, 17061.88 examples/s]\r\n",
      "Generating validation split: 1569 examples [00:00, 17039.51 examples/s]\r\n",
      "100%|█████████████████████████████████████| 1572/1572 [00:00<00:00, 1735.26it/s]\r\n",
      "Running loglikelihood requests: 100%|█████| 6288/6288 [1:16:53<00:00,  1.36it/s]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,peft=francescoocurcio/new_llama3.2-3B-log-ftn-lioa-3epoch_10k-sysprompt_no,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "| Tasks |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\r\n",
      "|-------|------:|------|-----:|--------|---|-----:|---|-----:|\r\n",
      "|logiqa2|      0|none  |     0|acc     |↑  |0.2990|±  |0.0116|\r\n",
      "|       |       |none  |     0|acc_norm|↑  |0.3073|±  |0.0116|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,peft=francescoocurcio/new_llama3.2-3B-log-ftn-lioa-3epoch_10k-sysprompt_no,parallelize=True \\\n",
    "    --tasks logiqa2 \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./llama \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47223c01",
   "metadata": {},
   "source": [
    "## Common Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c6be6",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bfc204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 17:28:33.194022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752341313.565819     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752341313.668111     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.47kB [00:00, 8.94MB/s]\r\n",
      "tokenizer_config.json: 54.7kB [00:00, 151MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 30.5MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 454/454 [00:00<00:00, 3.49MB/s]\r\n",
      "chat_template.jinja: 3.83kB [00:00, 2.73MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 2.24G/2.24G [00:11<00:00, 197MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 234/234 [00:00<00:00, 1.96MB/s]\r\n",
      "README.md: 100%|███████████████████████████████| 985/985 [00:00<00:00, 7.84MB/s]\r\n",
      "dataset_infos.json: 3.48kB [00:00, 21.8MB/s]\r\n",
      "coqa_train.parquet: 100%|██████████████████| 15.2M/15.2M [00:00<00:00, 30.3MB/s]\r\n",
      "coqa_validation.parquet: 100%|█████████████| 2.22M/2.22M [00:00<00:00, 5.89MB/s]\r\n",
      "Generating train split: 100%|█████| 7199/7199 [00:00<00:00, 25463.09 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 19099.22 examples/s]\r\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 8876.08it/s]\r\n",
      "Running generate_until requests:   0%|                  | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|██████| 500/500 [1:13:06<00:00,  8.77s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\r\n",
      "|-----|------:|------|-----:|------|---|-----:|---|-----:|\r\n",
      "|coqa |      3|none  |     0|em    |↑  |0.0992|±  |0.0128|\r\n",
      "|     |       |none  |     0|f1    |↑  |0.2817|±  |0.0143|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,parallelize=True \\\n",
    "    --tasks coqa \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./llama \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf09359",
   "metadata": {},
   "source": [
    "### Post Fine-Tuning\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201cd75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-12 21:45:17.805690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752356718.175928     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752356718.286035     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.47kB [00:00, 7.04MB/s]\r\n",
      "tokenizer_config.json: 54.7kB [00:00, 108MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 17.2M/17.2M [00:00<00:00, 28.5MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 454/454 [00:00<00:00, 3.90MB/s]\r\n",
      "chat_template.jinja: 3.83kB [00:00, 20.6MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 2.24G/2.24G [00:11<00:00, 191MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 234/234 [00:00<00:00, 2.51MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 817/817 [00:00<00:00, 7.34MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████▉| 97.3M/97.3M [00:00<00:00, 109MB/s]\r\n",
      "README.md: 100%|███████████████████████████████| 985/985 [00:00<00:00, 8.59MB/s]\r\n",
      "dataset_infos.json: 3.48kB [00:00, 21.2MB/s]\r\n",
      "coqa_train.parquet: 100%|██████████████████| 15.2M/15.2M [00:00<00:00, 29.2MB/s]\r\n",
      "coqa_validation.parquet: 100%|█████████████| 2.22M/2.22M [00:00<00:00, 6.17MB/s]\r\n",
      "Generating train split: 100%|█████| 7199/7199 [00:00<00:00, 28369.63 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 13158.77 examples/s]\r\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 9020.28it/s]\r\n",
      "Running generate_until requests:   0%|                  | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|██████| 500/500 [1:20:37<00:00,  9.68s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,peft=francescoocurcio/new_llama3.2-3B-log-ftn-csqa-3epoch_trainsplit-sysprompt_no,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\r\n",
      "|-----|------:|------|-----:|------|---|-----:|---|-----:|\r\n",
      "|coqa |      3|none  |     0|em    |↑  |0.2280|±  |0.0175|\r\n",
      "|     |       |none  |     0|f1    |↑  |0.3227|±  |0.0180|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Llama-3.2-3B-Instruct-bnb-4bit,peft=francescoocurcio/new_llama3.2-3B-log-ftn-csqa-3epoch_trainsplit-sysprompt_no,parallelize=True \\\n",
    "    --tasks coqa \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./llama \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cee386",
   "metadata": {},
   "source": [
    "# QWEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6291f141",
   "metadata": {},
   "source": [
    "## MATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b52e92",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c75c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 18:35:50.336811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752431750.552995     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752431750.613277     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.70kB [00:00, 7.98MB/s]\r\n",
      "tokenizer_config.json: 10.5kB [00:00, 36.3MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 40.6MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 143MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 21.5MB/s]\r\n",
      "added_tokens.json: 100%|███████████████████████| 707/707 [00:00<00:00, 5.08MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 614/614 [00:00<00:00, 4.44MB/s]\r\n",
      "chat_template.jinja: 4.67kB [00:00, 20.4MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 3.55G/3.55G [00:16<00:00, 213MB/s]\r\n",
      "generation_config.json: 100%|███████████████████| 237/237 [00:00<00:00, 605kB/s]\r\n",
      "README.md: 7.94kB [00:00, 24.2MB/s]\r\n",
      "main/train-00000-of-00001.parquet: 100%|███| 2.31M/2.31M [00:00<00:00, 4.89MB/s]\r\n",
      "main/test-00000-of-00001.parquet: 100%|██████| 419k/419k [00:00<00:00, 1.35MB/s]\r\n",
      "Generating train split: 100%|█████| 7473/7473 [00:00<00:00, 92299.09 examples/s]\r\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 163725.57 examples/s]\r\n",
      "100%|██████████████████████████████████████| 1319/1319 [00:05<00:00, 235.32it/s]\r\n",
      "Running generate_until requests:   0%|                 | 0/1319 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|████| 1319/1319 [7:42:30<00:00, 21.04s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.2047|±  |0.0111|\r\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.0227|±  |0.0041|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,parallelize=True \\\n",
    "    --tasks gsm8k \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./qwen \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2524bf",
   "metadata": {},
   "source": [
    "### Post Fine-Tuning\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c00f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 15:33:21.625103: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752507201.988085     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752507202.094223     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.70kB [00:00, 9.91MB/s]\r\n",
      "tokenizer_config.json: 10.5kB [00:00, 28.2MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 43.1MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 54.5MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 20.5MB/s]\r\n",
      "added_tokens.json: 100%|███████████████████████| 707/707 [00:00<00:00, 4.31MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 614/614 [00:00<00:00, 2.82MB/s]\r\n",
      "chat_template.jinja: 4.67kB [00:00, 11.6MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 3.55G/3.55G [00:18<00:00, 191MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 237/237 [00:00<00:00, 1.11MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 862/862 [00:00<00:00, 5.06MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "adapter_model.safetensors: 100%|██████████▉| 66.1M/66.1M [00:00<00:00, 82.7MB/s]\r\n",
      "README.md: 7.94kB [00:00, 18.3MB/s]\r\n",
      "main/train-00000-of-00001.parquet: 100%|███| 2.31M/2.31M [00:00<00:00, 4.10MB/s]\r\n",
      "main/test-00000-of-00001.parquet: 100%|██████| 419k/419k [00:00<00:00, 1.10MB/s]\r\n",
      "Generating train split: 100%|█████| 7473/7473 [00:00<00:00, 68760.44 examples/s]\r\n",
      "Generating test split: 100%|█████| 1319/1319 [00:00<00:00, 237274.27 examples/s]\r\n",
      "100%|██████████████████████████████████████| 1319/1319 [00:06<00:00, 211.22it/s]\r\n",
      "Running generate_until requests:   0%|                 | 0/1319 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|████| 1319/1319 [5:49:08<00:00, 15.88s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,peft=lorenagullone/QWEN4B_MATH_LoRA_r8_alpha16,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|     Filter     |n-shot|  Metric   |   |Value |   |Stderr|\r\n",
      "|-----|------:|----------------|-----:|-----------|---|-----:|---|-----:|\r\n",
      "|gsm8k|      3|flexible-extract|     5|exact_match|↑  |0.6520|±  |0.0131|\r\n",
      "|     |       |strict-match    |     5|exact_match|↑  |0.0986|±  |0.0082|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,peft=lorenagullone/QWEN4B_MATH_LoRA_r8_alpha16,parallelize=True \\\n",
    "    --tasks gsm8k \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./qwen \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1250b1",
   "metadata": {},
   "source": [
    "## Logic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03b4bc",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3a5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:58:22.408274: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752429502.773436     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752429502.876145     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.70kB [00:00, 7.16MB/s]\r\n",
      "tokenizer_config.json: 10.5kB [00:00, 48.8MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 49.0MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 134MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 21.2MB/s]\r\n",
      "added_tokens.json: 100%|███████████████████████| 707/707 [00:00<00:00, 5.19MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 614/614 [00:00<00:00, 4.64MB/s]\r\n",
      "chat_template.jinja: 4.67kB [00:00, 21.4MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 3.55G/3.55G [00:17<00:00, 200MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 237/237 [00:00<00:00, 2.04MB/s]\r\n",
      "README.md: 1.50kB [00:00, 6.82MB/s]\r\n",
      "logiqa2.py: 9.97kB [00:00, 34.2MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for baber/logiqa2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/baber/logiqa2\r\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\r\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n",
      "  warnings.warn(\r\n",
      "Downloading data: 100%|████████████████████| 14.0M/14.0M [00:00<00:00, 89.5MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.74M/1.74M [00:00<00:00, 53.7MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.77M/1.77M [00:00<00:00, 69.0MB/s]\r\n",
      "Generating train split: 12567 examples [00:01, 10468.01 examples/s]\r\n",
      "Generating test split: 1572 examples [00:00, 15113.99 examples/s]\r\n",
      "Generating validation split: 1569 examples [00:00, 15261.32 examples/s]\r\n",
      "100%|█████████████████████████████████████| 1572/1572 [00:00<00:00, 1776.20it/s]\r\n",
      "Running loglikelihood requests: 100%|█████| 6288/6288 [1:28:30<00:00,  1.18it/s]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "| Tasks |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\r\n",
      "|-------|------:|------|-----:|--------|---|-----:|---|-----:|\r\n",
      "|logiqa2|      0|none  |     0|acc     |↑  |0.2767|±  |0.0113|\r\n",
      "|       |       |none  |     0|acc_norm|↑  |0.2907|±  |0.0115|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,parallelize=True \\\n",
    "    --tasks logiqa2 \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./qwen \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c1c934",
   "metadata": {},
   "source": [
    "### Post Fine-Tuning\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e83761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 14:11:14.219628: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752502274.399430     115 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752502274.451035     115 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.70kB [00:00, 9.32MB/s]\r\n",
      "tokenizer_config.json: 10.5kB [00:00, 27.2MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 46.1MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 127MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 23.7MB/s]\r\n",
      "added_tokens.json: 100%|███████████████████████| 707/707 [00:00<00:00, 5.91MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 614/614 [00:00<00:00, 5.53MB/s]\r\n",
      "chat_template.jinja: 4.67kB [00:00, 23.9MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 3.55G/3.55G [00:19<00:00, 184MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 237/237 [00:00<00:00, 2.07MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 862/862 [00:00<00:00, 8.16MB/s]\r\n",
      "adapter_model.safetensors: 100%|███████████▉| 66.1M/66.1M [00:00<00:00, 141MB/s]\r\n",
      "README.md: 1.50kB [00:00, 9.87MB/s]\r\n",
      "logiqa2.py: 9.97kB [00:00, 35.2MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/datasets/load.py:1231: FutureWarning: The repository for baber/logiqa2 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/baber/logiqa2\r\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\r\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n",
      "  warnings.warn(\r\n",
      "Downloading data: 100%|████████████████████| 14.0M/14.0M [00:00<00:00, 71.3MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.74M/1.74M [00:00<00:00, 65.3MB/s]\r\n",
      "Downloading data: 100%|████████████████████| 1.77M/1.77M [00:01<00:00, 1.16MB/s]\r\n",
      "Generating train split: 12567 examples [00:01, 9649.31 examples/s]\r\n",
      "Generating test split: 1572 examples [00:00, 9923.19 examples/s] \r\n",
      "Generating validation split: 1569 examples [00:00, 10206.15 examples/s]\r\n",
      "100%|█████████████████████████████████████| 1572/1572 [00:00<00:00, 1651.69it/s]\r\n",
      "Running loglikelihood requests: 100%|█████| 6288/6288 [1:36:59<00:00,  1.08it/s]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,peft=lorenagullone/QWEN4B_LogicInference_LoRA_r8_alpha16,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "| Tasks |Version|Filter|n-shot| Metric |   |Value |   |Stderr|\r\n",
      "|-------|------:|------|-----:|--------|---|-----:|---|-----:|\r\n",
      "|logiqa2|      0|none  |     0|acc     |↑  |0.2754|±  |0.0113|\r\n",
      "|       |       |none  |     0|acc_norm|↑  |0.2913|±  |0.0115|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,peft=lorenagullone/QWEN4B_LogicInference_LoRA_r8_alpha16,parallelize=True \\\n",
    "    --tasks logiqa2 \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./qwen \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167aa921",
   "metadata": {},
   "source": [
    "## Common Sense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6e5132",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed0e6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-13 17:24:13.676891: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752427454.026618     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752427454.128797     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.70kB [00:00, 7.50MB/s]\r\n",
      "tokenizer_config.json: 10.5kB [00:00, 31.9MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 52.2MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 123MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 19.8MB/s]\r\n",
      "added_tokens.json: 100%|███████████████████████| 707/707 [00:00<00:00, 7.15MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 614/614 [00:00<00:00, 4.17MB/s]\r\n",
      "chat_template.jinja: 4.67kB [00:00, 19.6MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 3.55G/3.55G [00:17<00:00, 206MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 237/237 [00:00<00:00, 1.52MB/s]\r\n",
      "README.md: 100%|███████████████████████████████| 985/985 [00:00<00:00, 6.80MB/s]\r\n",
      "dataset_infos.json: 3.48kB [00:00, 21.3MB/s]\r\n",
      "coqa_train.parquet: 100%|██████████████████| 15.2M/15.2M [00:00<00:00, 31.9MB/s]\r\n",
      "coqa_validation.parquet: 100%|█████████████| 2.22M/2.22M [00:00<00:00, 6.86MB/s]\r\n",
      "Generating train split: 100%|█████| 7199/7199 [00:00<00:00, 36922.70 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 26867.27 examples/s]\r\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 7026.18it/s]\r\n",
      "Running generate_until requests:   0%|                  | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|██████| 500/500 [2:21:04<00:00, 16.93s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\r\n",
      "|-----|------:|------|-----:|------|---|-----:|---|-----:|\r\n",
      "|coqa |      3|none  |     0|em    |↑  |0.0000|±  |0.0000|\r\n",
      "|     |       |none  |     0|f1    |↑  |0.0003|±  |0.0003|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,parallelize=True \\\n",
    "    --tasks coqa \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./qwen \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cbc8a4",
   "metadata": {},
   "source": [
    "### Post Fine-Tuning\n",
    "\n",
    "Viene riportato l'eval del modello in seguito al primissimo training effettuato, quindi con una configurazione di LoRA pari ad: (r=8 ed $\\alpha=16$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43bfb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-14 07:03:33.373750: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1752476613.560064     112 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1752476613.620909     112 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "config.json: 1.70kB [00:00, 9.92MB/s]\r\n",
      "tokenizer_config.json: 10.5kB [00:00, 37.1MB/s]\r\n",
      "vocab.json: 2.78MB [00:00, 65.7MB/s]\r\n",
      "merges.txt: 1.67MB [00:00, 117MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 11.4M/11.4M [00:00<00:00, 17.2MB/s]\r\n",
      "added_tokens.json: 100%|███████████████████████| 707/707 [00:00<00:00, 5.80MB/s]\r\n",
      "special_tokens_map.json: 100%|█████████████████| 614/614 [00:00<00:00, 5.06MB/s]\r\n",
      "chat_template.jinja: 4.67kB [00:00, 18.6MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\r\n",
      "  warnings.warn(warning_msg)\r\n",
      "model.safetensors: 100%|████████████████████| 3.55G/3.55G [00:17<00:00, 199MB/s]\r\n",
      "generation_config.json: 100%|██████████████████| 237/237 [00:00<00:00, 1.51MB/s]\r\n",
      "adapter_config.json: 100%|█████████████████████| 862/862 [00:00<00:00, 8.31MB/s]\r\n",
      "/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\r\n",
      "  warnings.warn(\r\n",
      "adapter_model.safetensors: 100%|██████████▉| 66.1M/66.1M [00:00<00:00, 76.7MB/s]\r\n",
      "README.md: 100%|███████████████████████████████| 985/985 [00:00<00:00, 11.7MB/s]\r\n",
      "dataset_infos.json: 3.48kB [00:00, 20.7MB/s]\r\n",
      "coqa_train.parquet: 100%|██████████████████| 15.2M/15.2M [00:00<00:00, 24.7MB/s]\r\n",
      "coqa_validation.parquet: 100%|█████████████| 2.22M/2.22M [00:00<00:00, 5.20MB/s]\r\n",
      "Generating train split: 100%|█████| 7199/7199 [00:00<00:00, 35632.32 examples/s]\r\n",
      "Generating validation split: 100%|██| 500/500 [00:00<00:00, 24408.76 examples/s]\r\n",
      "100%|███████████████████████████████████████| 500/500 [00:00<00:00, 6280.14it/s]\r\n",
      "Running generate_until requests:   0%|                  | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\r\n",
      "  warnings.warn(\r\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\r\n",
      "  warnings.warn(\r\n",
      "Running generate_until requests: 100%|████████| 500/500 [43:34<00:00,  5.23s/it]\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n",
      "hf (pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,peft=lorenagullone/QWEN4B_CommonSenseQA_LoRA_r8_alpha16,parallelize=True,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 8\r\n",
      "|Tasks|Version|Filter|n-shot|Metric|   |Value |   |Stderr|\r\n",
      "|-----|------:|------|-----:|------|---|-----:|---|-----:|\r\n",
      "|coqa |      3|none  |     0|em    |↑  |0.0000|±  |0.0000|\r\n",
      "|     |       |none  |     0|f1    |↑  |0.0003|±  |0.0003|\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Qwen3-4B-unsloth-bnb-4bit,peft=lorenagullone/QWEN4B_CommonSenseQA_LoRA_r8_alpha16,parallelize=True \\\n",
    "    --tasks coqa \\\n",
    "    --device \"cuda\" \\\n",
    "    --batch_size 8 \\\n",
    "    --output_path ./qwen \\\n",
    "    --log_samples \\\n",
    "    --apply_chat_template \\\n",
    "    --trust_remote_code "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15269.990985,
   "end_time": "2025-07-11T18:15:15.804861",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-11T14:00:45.813876",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
