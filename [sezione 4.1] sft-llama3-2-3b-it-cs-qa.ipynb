{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caratteristiche del notebook\n",
    "\n",
    "- System_prompt presente = No\n",
    "- Dataset usato = CommonSense QA\n",
    "- Epoche = 3\n",
    "- Fine Tuning eseguito = Si - Repo -> francescoocurcio/new_llama3.2-3B-log-ftn-csqa-3epoch_trainsplit-sysprompt_no\n",
    "- Addestrato solo sulle risposte = No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione delle librerie e definizione delle costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:02:53.244503Z",
     "iopub.status.busy": "2025-05-10T11:02:53.244218Z",
     "iopub.status.idle": "2025-05-10T11:06:01.689944Z",
     "shell.execute_reply": "2025-05-10T11:06:01.688963Z",
     "shell.execute_reply.started": "2025-05-10T11:02:53.244482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:06:29.984132Z",
     "iopub.status.busy": "2025-05-10T11:06:29.983338Z",
     "iopub.status.idle": "2025-05-10T11:07:01.047671Z",
     "shell.execute_reply": "2025-05-10T11:07:01.046904Z",
     "shell.execute_reply.started": "2025-05-10T11:06:29.984101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT DELLE LIBRERIE\n",
    "#########################\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Mostra tutte le colonne\n",
    "pd.set_option('display.width', None)        # Non tronca l'output a una larghezza fissa\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from unsloth import FastLanguageModel, to_sharegpt\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "#########################\n",
    "# COSTANTI\n",
    "#########################\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "OUTPUT_REPO = \"francescoocurcio/new_llama3.2-3B-log-ftn-csqa-3epoch_trainsplit-sysprompt_no\"\n",
    "SAVE_DIRECTORY = \"/kaggle/working/new_llama3.2-3B-log-ftn-csqa-3epoch_trainsplit-sysprompt_no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:07:07.934293Z",
     "iopub.status.busy": "2025-05-10T11:07:07.933616Z",
     "iopub.status.idle": "2025-05-10T11:07:08.685337Z",
     "shell.execute_reply": "2025-05-10T11:07:08.684308Z",
     "shell.execute_reply.started": "2025-05-10T11:07:07.934262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selezione e configurazione del modello: Llama3.2 3B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:07:34.455324Z",
     "iopub.status.busy": "2025-05-10T11:07:34.454417Z",
     "iopub.status.idle": "2025-05-10T11:09:12.094387Z",
     "shell.execute_reply": "2025-05-10T11:09:12.093576Z",
     "shell.execute_reply.started": "2025-05-10T11:07:34.455285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    "    # token = \"hf...\" #Use one if using gated models like meta-llama/Llama....\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "#PEFT = Parameter Efficient Fine Tuning\n",
    "model = FastLanguageModel.get_peft_model( #Modello quantizzato\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:10:00.742897Z",
     "iopub.status.busy": "2025-05-10T11:10:00.742566Z",
     "iopub.status.idle": "2025-05-10T11:10:03.573956Z",
     "shell.execute_reply": "2025-05-10T11:10:03.573329Z",
     "shell.execute_reply.started": "2025-05-10T11:10:00.742872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"tau/commonsense_qa\", split=\"train\").to_pandas()\n",
    "train_data = train_data[['question','choices','answerKey']]\n",
    "#print(train_data.iloc[0])\n",
    "\n",
    "def format_choices_column(df, new_col = \"answers\"):\n",
    "    def format_choices(row):\n",
    "        labels = list(row[\"choices\"][\"label\"])\n",
    "        texts = list(row[\"choices\"][\"text\"])\n",
    "        return \"\\n\".join([f\"{label}){text}\" for label, text in zip(labels, texts)])\n",
    "    df[new_col] = df.apply(format_choices,axis=1)\n",
    "    return df       \n",
    "\n",
    "df = format_choices_column(train_data)\n",
    "\n",
    "def expand_answer_key(row):\n",
    "    labels = list(row[\"choices\"][\"label\"])\n",
    "    texts = list(row[\"choices\"][\"text\"])\n",
    "    key = row[\"answerKey\"]\n",
    "\n",
    "    if key in labels:\n",
    "        idx = labels.index(key)\n",
    "        return f\"{labels[idx]}) {texts[idx]}\"\n",
    "    else:\n",
    "        return None  # oppure \"Risposta non disponibile\"\n",
    "\n",
    "df[\"answerKey\"] = df.apply(expand_answer_key, axis=1)\n",
    "df = df[[\"question\",\"answers\",\"answerKey\"]]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:10:49.738965Z",
     "iopub.status.busy": "2025-05-10T11:10:49.738137Z",
     "iopub.status.idle": "2025-05-10T11:10:49.917765Z",
     "shell.execute_reply": "2025-05-10T11:10:49.916876Z",
     "shell.execute_reply.started": "2025-05-10T11:10:49.738930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"Answer the following question.[[:\\n{question}\\n{answers}]]\",\n",
    "    output_column_name = \"answerKey\",\n",
    "    conversation_extension = 1, #Select more to handle longer conversations\n",
    ")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opzione di aggiunta per il system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#system_message = {\n",
    "#    \"from\": \"system\",\n",
    "#    \"value\": (\n",
    "#        \"You are a math expert. You will be given a mathematical problem to solve. \"\n",
    "#        \"Your aim is to first provide a step-by-step explanation of the solution \"\n",
    "#        \"(integrating the formulae in LaTeX syntax) and then to conclude with a clear and concise final answer.\"\n",
    "#    )\n",
    "#}\n",
    "\n",
    "# Aggiungilo all'inizio di ogni conversazione\n",
    "#def add_system_message(example):\n",
    "#    conversation = example[\"conversations\"]\n",
    "#    return {\n",
    "#        \"conversations\": [system_message] + conversation\n",
    "#    }\n",
    "\n",
    "# Applica la funzione a tutto il dataset\n",
    "#dataset = dataset.map(add_system_message)\n",
    "#print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costruzione dataset finale conversazionale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:14:52.855876Z",
     "iopub.status.busy": "2025-05-10T11:14:52.855337Z",
     "iopub.status.idle": "2025-05-10T11:14:53.597354Z",
     "shell.execute_reply": "2025-05-10T11:14:53.596661Z",
     "shell.execute_reply.started": "2025-05-10T11:14:52.855834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:14:55.198980Z",
     "iopub.status.busy": "2025-05-10T11:14:55.198186Z",
     "iopub.status.idle": "2025-05-10T11:14:56.017501Z",
     "shell.execute_reply": "2025-05-10T11:14:56.016862Z",
     "shell.execute_reply.started": "2025-05-10T11:14:55.198946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:16:00.629210Z",
     "iopub.status.busy": "2025-05-10T11:16:00.628503Z",
     "iopub.status.idle": "2025-05-10T11:16:00.633850Z",
     "shell.execute_reply": "2025-05-10T11:16:00.633103Z",
     "shell.execute_reply.started": "2025-05-10T11:16:00.629187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "#Anche dopo aver cambiato inizialmente queste operazioni il tokenizer a fronte delle\n",
    "#operazioni eseguite è come se eseguisse una sorta di riformattazione\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:16:03.027161Z",
     "iopub.status.busy": "2025-05-10T11:16:03.026892Z",
     "iopub.status.idle": "2025-05-10T11:16:03.035028Z",
     "shell.execute_reply": "2025-05-10T11:16:03.034307Z",
     "shell.execute_reply.started": "2025-05-10T11:16:03.027140Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = dataset[0]['text']\n",
    "tokenized = tokenizer(text, return_tensors=\"pt\", return_attention_mask=True)\n",
    "input_ids = tokenized[\"input_ids\"][0]\n",
    "\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for i, (token_id, token_str) in enumerate(zip(input_ids, decoded_tokens)):\n",
    "    print(f\"{i:03d} | Token ID: {token_id.item():>6} | Token: {repr(token_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestramento del modello tramite LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:16:15.183392Z",
     "iopub.status.busy": "2025-05-10T11:16:15.182831Z",
     "iopub.status.idle": "2025-05-10T11:16:18.539214Z",
     "shell.execute_reply": "2025-05-10T11:16:18.538391Z",
     "shell.execute_reply.started": "2025-05-10T11:16:15.183360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            loss = logs[\"loss\"]\n",
    "            step = state.global_step\n",
    "            self.train_losses.append((step, loss))\n",
    "            print(f\"📉 Step {step} - Loss: {loss:.4f}\")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "loss_callback = LossLoggerCallback()\n",
    "\n",
    "#model.config.use_cache = False\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    do_train                    = True,\n",
    "\n",
    "    dataset_text_field          = \"text\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 8,\n",
    "\n",
    "    num_train_epochs            = 3,   # Epoche complete\n",
    "    #max_steps                   = 10,\n",
    "    \n",
    "    learning_rate               = 2e-4,\n",
    "    lr_scheduler_type           = \"linear\",\n",
    "    logging_strategy            = 'steps',\n",
    "    logging_steps               = 20,\n",
    "    # 💾 Salvataggio\n",
    "    save_strategy               = 'epoch',\n",
    "    #save_steps                  = 200,\n",
    "\n",
    "    warmup_steps                = 40,\n",
    "\n",
    "    optim                       = \"adamw_8bit\",\n",
    "    seed                        = 42,\n",
    "\n",
    "    fp16                        = not is_bfloat16_supported(),\n",
    "    bf16                        = is_bfloat16_supported(),\n",
    "\n",
    "    weight_decay                = 0.01,\n",
    "    report_to                   = \"none\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    dataset_num_proc   = 2,\n",
    "    max_seq_length     = MAX_SEQ_LENGTH,\n",
    "    train_dataset      = dataset,\n",
    "    args               = training_args,\n",
    "    data_collator      = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing            = False,\n",
    "    callbacks          = [loss_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:16:22.138770Z",
     "iopub.status.busy": "2025-05-10T11:16:22.138098Z",
     "iopub.status.idle": "2025-05-10T11:17:14.689852Z",
     "shell.execute_reply": "2025-05-10T11:17:14.688344Z",
     "shell.execute_reply.started": "2025-05-10T11:16:22.138738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Avvio addestramento LoRA...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:49:46.044263Z",
     "iopub.status.busy": "2025-05-10T10:49:46.043627Z",
     "iopub.status.idle": "2025-05-10T10:49:46.255531Z",
     "shell.execute_reply": "2025-05-10T10:49:46.254820Z",
     "shell.execute_reply.started": "2025-05-10T10:49:46.044239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir $SAVE_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PUSH MODELLO LoRA + TOKENIZER su HUGGING FACE\n",
    "print(\"🔄 Caricamento del modello e del tokenizer in corso...\")\n",
    "model.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "tokenizer.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "\n",
    "print(f\"✅ Modello caricato correttamente su: {OUTPUT_REPO}\\n\")\n",
    "print(f\"✅ Tokenizer caricato correttamente su: {OUTPUT_REPO}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:50:26.486446Z",
     "iopub.status.busy": "2025-05-10T10:50:26.486167Z",
     "iopub.status.idle": "2025-05-10T10:50:27.868676Z",
     "shell.execute_reply": "2025-05-10T10:50:27.867800Z",
     "shell.execute_reply.started": "2025-05-10T10:50:26.486426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SALVATAGGIO MODELLO LoRA + TOKENIZER\n",
    "print(\"💾 Salvataggio del modello e tokenizer...\")\n",
    "trainer.model.save_pretrained(SAVE_DIRECTORY)\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "print(f\"✅ Modello LoRA salvato in: {SAVE_DIRECTORY}\")\n",
    "\n",
    "# SALVATAGGIO LOSS (è stato usato LossLoggerCallback)\n",
    "loss_path = os.path.join(SAVE_DIRECTORY, \"loss_log.csv\")\n",
    "\n",
    "if hasattr(trainer.callback_handler.callbacks[0], \"train_losses\"):\n",
    "    print(\"📊 Esportazione delle loss in CSV...\")\n",
    "    losses = trainer.callback_handler.callbacks[0].train_losses\n",
    "    loss_df = pd.DataFrame(losses, columns=[\"step\", \"loss\"])\n",
    "    loss_df.to_csv(loss_path, index=False)\n",
    "    print(f\"✅ Loss salvate in {loss_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Nessuna loss trovata nei callback!\")\n",
    "\n",
    "\n",
    "# VISUALIZZAZIONE LOSS (solo se il file esiste)\n",
    "if os.path.exists(loss_path):\n",
    "    print(\"📈 Visualizzazione della training loss...\")\n",
    "    loss_df = pd.read_csv(loss_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=loss_df, x=\"step\", y=\"loss\", marker=\"o\")\n",
    "\n",
    "    plt.title(\"Andamento della Training Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"❌ File {loss_path} non trovato. Salta visualizzazione.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizzazione della loss di addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:51:18.791507Z",
     "iopub.status.busy": "2025-05-10T10:51:18.791259Z",
     "iopub.status.idle": "2025-05-10T10:51:19.060204Z",
     "shell.execute_reply": "2025-05-10T10:51:19.059547Z",
     "shell.execute_reply.started": "2025-05-10T10:51:18.791491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crate_loss_chart(json_file):\n",
    "    # Carica il file JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Estrai i dati di interesse\n",
    "    log_history = data.get(\"log_history\", [])\n",
    "    steps = [entry[\"step\"] for entry in log_history]\n",
    "    losses = [entry[\"loss\"] for entry in log_history]\n",
    "    \n",
    "    # Crea un DataFrame\n",
    "    df = pd.DataFrame({\"Step\": steps, \"Loss\": losses})\n",
    "    \n",
    "    # Crea il grafico a linee\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x=\"Step\", y=\"Loss\", marker='o')\n",
    "    plt.title(\"Andamento della Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # Rotazione delle etichette e selezione dei ticks\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.locator_params(axis='x', nbins=10)  # Mostra solo 10 ticks sull'asse x\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()  # Migliora la disposizione degli elementi\n",
    "    plt.show()\n",
    "\n",
    "loss_path = \"/kaggle/working/trainer_output/checkpoint-912/trainer_state.json\"\n",
    "crate_loss_chart(loss_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
