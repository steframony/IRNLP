{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caratteristiche del notebook\n",
    "\n",
    "- System_prompt presente = No\n",
    "- Dataset usato = Yale FOLIO\n",
    "- Epoche = 3\n",
    "- Fine Tuning eseguito = Si - Repo -> francescoocurcio/new_llama3.2-3B-log-ftn-folio-3epoch_trainsplit-sysprompt_no\n",
    "- Addestrato solo sulle risposte = No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione delle librerie e definizione delle costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:08:38.648348Z",
     "iopub.status.busy": "2025-05-10T17:08:38.648056Z",
     "iopub.status.idle": "2025-05-10T17:12:50.462126Z",
     "shell.execute_reply": "2025-05-10T17:12:50.461005Z",
     "shell.execute_reply.started": "2025-05-10T17:08:38.648327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:13:12.013081Z",
     "iopub.status.busy": "2025-05-10T17:13:12.012761Z",
     "iopub.status.idle": "2025-05-10T17:13:55.647708Z",
     "shell.execute_reply": "2025-05-10T17:13:55.646844Z",
     "shell.execute_reply.started": "2025-05-10T17:13:12.013054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT DELLE LIBRERIE\n",
    "#########################\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Mostra tutte le colonne\n",
    "pd.set_option('display.width', None)        # Non tronca l'output a una larghezza fissa\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from unsloth import FastLanguageModel, to_sharegpt\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "#########################\n",
    "# COSTANTI\n",
    "#########################\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "OUTPUT_REPO = \"francescoocurcio/new_llama3.2-3B-log-ftn-folio-3epoch_trainsplit-sysprompt_no\"\n",
    "SAVE_DIRECTORY = \"/kaggle/working/new_llama3.2-3B-log-ftn-folio-3epoch_trainsplit-sysprompt_no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:13:58.016889Z",
     "iopub.status.busy": "2025-05-10T17:13:58.016315Z",
     "iopub.status.idle": "2025-05-10T17:13:58.788191Z",
     "shell.execute_reply": "2025-05-10T17:13:58.787428Z",
     "shell.execute_reply.started": "2025-05-10T17:13:58.016863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selezione e configurazione del modello: Llama3.2 3B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:14:05.696027Z",
     "iopub.status.busy": "2025-05-10T17:14:05.695723Z",
     "iopub.status.idle": "2025-05-10T17:15:48.698079Z",
     "shell.execute_reply": "2025-05-10T17:15:48.697412Z",
     "shell.execute_reply.started": "2025-05-10T17:14:05.696002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    "    # token = \"hf...\" #Use one if using gated models like meta-llama/Llama....\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "#PEFT = Parameter Efficient Fine Tuning\n",
    "model = FastLanguageModel.get_peft_model( #Modello quantizzato\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:32:51.029997Z",
     "iopub.status.busy": "2025-05-10T17:32:51.029098Z",
     "iopub.status.idle": "2025-05-10T17:32:52.656900Z",
     "shell.execute_reply": "2025-05-10T17:32:52.656076Z",
     "shell.execute_reply.started": "2025-05-10T17:32:51.029960Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = load_dataset(\"yale-nlp/FOLIO\")['train'].to_pandas()\n",
    "\n",
    "def map_logical_labels(df):\n",
    "    \"\"\"\n",
    "    Replace the 'label' column values in a DataFrame with natural language explanations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with a 'label' column containing 'True', 'False', or 'Uncertain'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with descriptive natural language labels.\n",
    "    \"\"\"\n",
    "    label_mapping = {\n",
    "        \"True\": \"Based on the information provided the conclusion logically follows from the premises\",\n",
    "        \"False\": \"Based on the information provided the conclusion does not logically follow from the premises.\",\n",
    "        \"Uncertain\": \"Based on the information provided the conclusion does not necessarily follow from the premises. Therefore, the inference is uncertain.\"\n",
    "    }\n",
    "\n",
    "    df = df.copy()\n",
    "    df['label'] = df['label'].map(label_mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = map_logical_labels(df)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:37:26.750416Z",
     "iopub.status.busy": "2025-05-10T17:37:26.749700Z",
     "iopub.status.idle": "2025-05-10T17:37:26.771849Z",
     "shell.execute_reply": "2025-05-10T17:37:26.771088Z",
     "shell.execute_reply.started": "2025-05-10T17:37:26.750385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def add_instruction_response_columns(df):\n",
    "    \"\"\"\n",
    "    Adds 'INSTRUCTION' and 'RESPONSE' columns to a FOLIO-style DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with columns ['premises', 'premises-FOL', 'conclusion', 'conclusion-FOL', 'label']\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with added 'INSTRUCTION' and 'RESPONSE' columns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"INSTRUCTION\"] = (\n",
    "        \"Given the following premises in Natural Language: \\n\" + df[\"premises\"] +\n",
    "        \" And its version in First Order Logic (FOL): \\n\" + df[\"premises-FOL\"] +\n",
    "        \". Give the conclusion in Natural Language and in FOL. Tell also if the conclusion logically follows or not or is uncertain.\"\n",
    "    )\n",
    "\n",
    "    df[\"RESPONSE\"] = (\n",
    "        \"Here's the conclusion in FOL: \" + df[\"conclusion-FOL\"] +\n",
    "        \" and here is the conclusion in Natural Language: \" + df[\"conclusion\"] + df[\"label\"]\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "df = add_instruction_response_columns(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:39:38.993250Z",
     "iopub.status.busy": "2025-05-10T17:39:38.992509Z",
     "iopub.status.idle": "2025-05-10T17:39:39.005380Z",
     "shell.execute_reply": "2025-05-10T17:39:39.004482Z",
     "shell.execute_reply.started": "2025-05-10T17:39:38.993224Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df[['INSTRUCTION', 'RESPONSE']]\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:40:31.564278Z",
     "iopub.status.busy": "2025-05-10T17:40:31.563926Z",
     "iopub.status.idle": "2025-05-10T17:40:31.646968Z",
     "shell.execute_reply": "2025-05-10T17:40:31.646194Z",
     "shell.execute_reply.started": "2025-05-10T17:40:31.564257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"[[{INSTRUCTION}]]\",\n",
    "    output_column_name = \"RESPONSE\",\n",
    "    conversation_extension = 1, #Select more to handle longer conversations\n",
    ")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opzione di aggiunta per il system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#system_message = {\n",
    "#    \"from\": \"system\",\n",
    "#    \"value\": (\n",
    "#        \"You are a math expert. You will be given a mathematical problem to solve. \"\n",
    "#        \"Your aim is to first provide a step-by-step explanation of the solution \"\n",
    "#        \"(integrating the formulae in LaTeX syntax) and then to conclude with a clear and concise final answer.\"\n",
    "#    )\n",
    "#}\n",
    "\n",
    "# Aggiungilo all'inizio di ogni conversazione\n",
    "#def add_system_message(example):\n",
    "#    conversation = example[\"conversations\"]\n",
    "#    return {\n",
    "#        \"conversations\": [system_message] + conversation\n",
    "#    }\n",
    "\n",
    "# Applica la funzione a tutto il dataset\n",
    "#dataset = dataset.map(add_system_message)\n",
    "#print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costruzione dataset finale conversazionale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:41:10.479653Z",
     "iopub.status.busy": "2025-05-10T17:41:10.479052Z",
     "iopub.status.idle": "2025-05-10T17:41:11.210277Z",
     "shell.execute_reply": "2025-05-10T17:41:11.209351Z",
     "shell.execute_reply.started": "2025-05-10T17:41:10.479630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:41:14.142892Z",
     "iopub.status.busy": "2025-05-10T17:41:14.142228Z",
     "iopub.status.idle": "2025-05-10T17:41:14.510921Z",
     "shell.execute_reply": "2025-05-10T17:41:14.510041Z",
     "shell.execute_reply.started": "2025-05-10T17:41:14.142863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:41:22.231864Z",
     "iopub.status.busy": "2025-05-10T17:41:22.231555Z",
     "iopub.status.idle": "2025-05-10T17:41:22.236973Z",
     "shell.execute_reply": "2025-05-10T17:41:22.236153Z",
     "shell.execute_reply.started": "2025-05-10T17:41:22.231841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "#Anche dopo aver cambiato inizialmente queste operazioni il tokenizer a fronte delle\n",
    "#operazioni eseguite è come se eseguisse una sorta di riformattazione\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:41:25.727912Z",
     "iopub.status.busy": "2025-05-10T17:41:25.727231Z",
     "iopub.status.idle": "2025-05-10T17:41:25.739896Z",
     "shell.execute_reply": "2025-05-10T17:41:25.739085Z",
     "shell.execute_reply.started": "2025-05-10T17:41:25.727886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = dataset[0]['text']\n",
    "tokenized = tokenizer(text, return_tensors=\"pt\", return_attention_mask=True)\n",
    "input_ids = tokenized[\"input_ids\"][0]\n",
    "\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for i, (token_id, token_str) in enumerate(zip(input_ids, decoded_tokens)):\n",
    "    print(f\"{i:03d} | Token ID: {token_id.item():>6} | Token: {repr(token_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestramento del modello tramite LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:42:55.848459Z",
     "iopub.status.busy": "2025-05-10T17:42:55.847923Z",
     "iopub.status.idle": "2025-05-10T17:42:58.563448Z",
     "shell.execute_reply": "2025-05-10T17:42:58.562793Z",
     "shell.execute_reply.started": "2025-05-10T17:42:55.848433Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            loss = logs[\"loss\"]\n",
    "            step = state.global_step\n",
    "            self.train_losses.append((step, loss))\n",
    "            print(f\"📉 Step {step} - Loss: {loss:.4f}\")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "loss_callback = LossLoggerCallback()\n",
    "\n",
    "#model.config.use_cache = False\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    do_train                    = True,\n",
    "\n",
    "    dataset_text_field          = \"text\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 8,\n",
    "\n",
    "    num_train_epochs            = 3,   # Epoche complete\n",
    "    #max_steps                   = 10,\n",
    "    \n",
    "    learning_rate               = 2e-4,\n",
    "    lr_scheduler_type           = \"linear\",\n",
    "    logging_strategy            = 'steps',\n",
    "    logging_steps               = 20,\n",
    "    # 💾 Salvataggio\n",
    "    save_strategy               = 'epoch',\n",
    "    #save_steps                  = 200,\n",
    "\n",
    "    warmup_steps                = 40,\n",
    "\n",
    "    optim                       = \"adamw_8bit\",\n",
    "    seed                        = 42,\n",
    "\n",
    "    fp16                        = not is_bfloat16_supported(),\n",
    "    bf16                        = is_bfloat16_supported(),\n",
    "\n",
    "    weight_decay                = 0.01,\n",
    "    report_to                   = \"none\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    dataset_num_proc   = 2,\n",
    "    max_seq_length     = MAX_SEQ_LENGTH,\n",
    "    train_dataset      = dataset,\n",
    "    args               = training_args,\n",
    "    data_collator      = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing            = False,\n",
    "    callbacks          = [loss_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T17:43:05.058855Z",
     "iopub.status.busy": "2025-05-10T17:43:05.058509Z",
     "iopub.status.idle": "2025-05-10T18:14:51.425641Z",
     "shell.execute_reply": "2025-05-10T18:14:51.424962Z",
     "shell.execute_reply.started": "2025-05-10T17:43:05.058830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Avvio addestramento LoRA...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvataggio del modello e visualizzazione della loss di addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T18:22:27.437246Z",
     "iopub.status.busy": "2025-05-10T18:22:27.436344Z",
     "iopub.status.idle": "2025-05-10T18:22:27.670420Z",
     "shell.execute_reply": "2025-05-10T18:22:27.668641Z",
     "shell.execute_reply.started": "2025-05-10T18:22:27.437221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir $SAVE_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T18:22:54.199702Z",
     "iopub.status.busy": "2025-05-10T18:22:54.199375Z",
     "iopub.status.idle": "2025-05-10T18:23:04.466695Z",
     "shell.execute_reply": "2025-05-10T18:23:04.465935Z",
     "shell.execute_reply.started": "2025-05-10T18:22:54.199676Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PUSH MODELLO LoRA + TOKENIZER su HUGGING FACE\n",
    "print(\"🔄 Caricamento del modello e del tokenizer in corso...\")\n",
    "model.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "tokenizer.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "\n",
    "print(f\"✅ Modello caricato correttamente su: {OUTPUT_REPO}\\n\")\n",
    "print(f\"✅ Tokenizer caricato correttamente su: {OUTPUT_REPO}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T18:23:11.124444Z",
     "iopub.status.busy": "2025-05-10T18:23:11.123834Z",
     "iopub.status.idle": "2025-05-10T18:23:12.599705Z",
     "shell.execute_reply": "2025-05-10T18:23:12.598887Z",
     "shell.execute_reply.started": "2025-05-10T18:23:11.124421Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SALVATAGGIO MODELLO LoRA + TOKENIZER\n",
    "print(\"💾 Salvataggio del modello e tokenizer...\")\n",
    "trainer.model.save_pretrained(SAVE_DIRECTORY)\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "print(f\"✅ Modello LoRA salvato in: {SAVE_DIRECTORY}\")\n",
    "\n",
    "#Visualizzazione loss di addestramento\n",
    "def crate_loss_chart(json_file):\n",
    "    # Carica il file JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Estrai i dati di interesse\n",
    "    log_history = data.get(\"log_history\", [])\n",
    "    steps = [entry[\"step\"] for entry in log_history]\n",
    "    losses = [entry[\"loss\"] for entry in log_history]\n",
    "    \n",
    "    # Crea un DataFrame\n",
    "    df = pd.DataFrame({\"Step\": steps, \"Loss\": losses})\n",
    "    \n",
    "    # Crea il grafico a linee\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x=\"Step\", y=\"Loss\", marker='o')\n",
    "    plt.title(\"Andamento della Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # Rotazione delle etichette e selezione dei ticks\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.locator_params(axis='x', nbins=10)  # Mostra solo 10 ticks sull'asse x\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()  # Migliora la disposizione degli elementi\n",
    "    plt.show()\n",
    "\n",
    "loss_path = \"/kaggle/working/trainer_output/checkpoint-93/trainer_state.json\"\n",
    "crate_loss_chart(loss_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
