{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Knowledge Distillation\n",
    "In questo notebook viene riportato un esperimento riguardo alla distillazione inversa, ossia trasferire la conoscenza degli adattatori lora di un modello piccolo, ad adattatori lora per un modello grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:57:55.988133Z",
     "iopub.status.busy": "2025-06-02T17:57:55.987553Z",
     "iopub.status.idle": "2025-06-02T17:59:18.569389Z",
     "shell.execute_reply": "2025-06-02T17:59:18.568673Z",
     "shell.execute_reply.started": "2025-06-02T17:57:55.988109Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -U -q bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Librerie ###############\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:59:23.022141Z",
     "iopub.status.busy": "2025-06-02T17:59:23.021838Z",
     "iopub.status.idle": "2025-06-02T17:59:26.473933Z",
     "shell.execute_reply": "2025-06-02T17:59:26.473166Z",
     "shell.execute_reply.started": "2025-06-02T17:59:23.022112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading big model\n",
    "In questa sezione viene importato il modello grande alla quale deve essere trasferita la conoscenza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:59:26.475585Z",
     "iopub.status.busy": "2025-06-02T17:59:26.475191Z",
     "iopub.status.idle": "2025-06-02T18:01:48.304040Z",
     "shell.execute_reply": "2025-06-02T18:01:48.303427Z",
     "shell.execute_reply.started": "2025-06-02T17:59:26.475559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-02 17:59:36.630683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748887176.865972      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748887176.928537      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello e del tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a650d617013e4696be0f4a4fa4ea652c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9550ebd1fb5448bc9237bc6ead943430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98853692fea2438cac37419bd530a399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec7327301e74a94beab1cfb2fbdd224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5e9caa3fd44e66a438d56bad95c69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe828f901934fda83c04425fdcd1a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a76c2f6e694413a49dad9e82c0afb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cffd27388c94ea09f2b5ffe96dfc995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e8ae64ca20445a96a4b83f532bc5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fecb81e214b45258029bb2c14ccb473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f279d4317b417f95eb53cd1c9b2ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb31a8b57074f2daa75fade82f78d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444783762ca64f2980760be573a61f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-2-9b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "print(\"Caricamento del modello e del tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model_9b = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    token=token,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:02:01.107807Z",
     "iopub.status.busy": "2025-06-02T18:02:01.107061Z",
     "iopub.status.idle": "2025-06-02T18:02:01.120118Z",
     "shell.execute_reply": "2025-06-02T18:02:01.119418Z",
     "shell.execute_reply.started": "2025-06-02T18:02:01.107782Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\nIf $\\\\mathbf{a} = \\x08egin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\end{pmatrix},$ then find the vector $\\\\mathbf{v}$ such that $\\\\mathbf{a} \\\\cdot \\\\mathbf{v} = 2$ and $\\\\mathbf{a} \\times \\\\mathbf{v} = \\x08egin{pmatrix} 1 \\\\ -2 \\\\ 1 \\\\end{pmatrix}.$<end_of_turn>\\n<start_of_turn>model\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text = \"\"\"If $\\mathbf{a} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix},$ then find the vector $\\mathbf{v}$ such that $\\mathbf{a} \\cdot \\mathbf{v} = 2$ and $\\mathbf{a} \\times \\mathbf{v} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}.$\"\"\"\n",
    "chat_input = tokenizer.apply_chat_template(\n",
    "            [{\"role\": \"user\", \"content\": input_text}], \n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    ")\n",
    "if chat_input.startswith(\"<bos>\"):\n",
    "        chat_input = chat_input[len(\"<bos>\"):]\n",
    "chat_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:05:38.534386Z",
     "iopub.status.busy": "2025-06-02T18:05:38.533591Z",
     "iopub.status.idle": "2025-06-02T18:06:53.246538Z",
     "shell.execute_reply": "2025-06-02T18:06:53.245701Z",
     "shell.execute_reply.started": "2025-06-02T18:05:38.534364Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "**Understanding the Problem**\n",
      "\n",
      "* **Dot Product:**  The dot product of two vectors gives us a scalar value.  The equation $\\mathbf{a} \\cdot \\mathbf{v} = 2$ tells us the magnitude of the projection of $\\mathbf{v}$ onto $\\mathbf{a}$ is 2.\n",
      "* **Cross Product:** The cross product of two vectors gives us a new vector that is perpendicular to both original vectors. The equation $\\mathbf{a} \\times \\mathbf{v} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$ tells us the direction and magnitude of the vector resulting from the cross product.\n",
      "\n",
      "**Solving for v**\n",
      "\n",
      "1. **Expressing v:** Let $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$.\n",
      "\n",
      "2. **Dot Product:**\n",
      "   *  $\\mathbf{a} \\cdot \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = x + y + z = 2$\n",
      "\n",
      "3. **Cross Product:**\n",
      "   * $\\mathbf{a} \\times \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\times \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} (1)(z) - (1)(y) \\\\ (1)(x) - (1)(z) \\\\ (1)(y) - (1)(x) \\end{pmatrix} = \\begin{pmatrix} z - y \\\\ x - z \\\\ y - x \\end{pmatrix}$\n",
      "   * We know this must equal $\\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$.  Therefore:\n",
      "      * $z - y = 1$\n",
      "      * $x - z = -2$\n",
      "      * $y - x = 1$\n",
      "\n",
      "4. **Solving the System of Equations:** We have three equations and three unknowns.  Solve the system of equations to find the values of *x*, *y*, and *z*.\n",
      "\n",
      "**Note:** There might be multiple solutions for $\\mathbf{v}$ depending on the specific values you get from solving the system of equations. \n",
      "\n",
      "\n",
      "Let me know if you'd like me to work through the steps of solving the system of equations.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model_9b.device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "        \n",
    "    outputs = model_9b.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024\n",
    "    )\n",
    "    \n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "input_length = len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True))\n",
    "generated_text = full_output[input_length:].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto definiamo le varie matrici LoRA del modello grande, matrici che dovranno essere opportunamente costruite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T18:07:32.204282Z",
     "iopub.status.busy": "2025-06-02T18:07:32.203951Z",
     "iopub.status.idle": "2025-06-02T18:07:32.863702Z",
     "shell.execute_reply": "2025-06-02T18:07:32.863081Z",
     "shell.execute_reply.started": "2025-06-02T18:07:32.204231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma2ForCausalLM(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-41): 42 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3584, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3584, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,                # rango di decomposizione \n",
    "    lora_alpha=16,      # fattore di scala per la matrice LoRA\n",
    "    lora_dropout=0,     # probabilità di dropout per la LoRA\n",
    "    bias=\"none\",  \n",
    "    target_modules             = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                                  \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model_9b, lora_config)\n",
    "peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load small model\n",
    "Caricamento del modello piccolo, modello sottoposto precedentemente ad un SFT in un task predefinito, in questo caso è MATH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:07:38.137741Z",
     "iopub.status.busy": "2025-06-02T18:07:38.137459Z",
     "iopub.status.idle": "2025-06-02T18:08:08.941994Z",
     "shell.execute_reply": "2025-06-02T18:08:08.941427Z",
     "shell.execute_reply.started": "2025-06-02T18:07:38.137723Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello e del tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc658351ce5947a796296bdf8427ac99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a121d875476245188ddeddbdbeb60c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c601a5324d754843b5f5085ebdacbadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f030ca4d7dd04e6a9f679aa11729742c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2352757bb64454aa70a237ee7e3998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e39f1992e7b4e6bb53d091793ba6de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fc475af9794529890e222a908eb85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02535c2365c4d109aceb15366d64070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91d79fa7c3b448ba6f9c85213500e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba5d18c84fe24f5c88d14156c6136ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394f80a3a72d4747bb471ad4883b8296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "print(\"Caricamento del modello e del tokenizer...\")\n",
    "tokenizer_2b = AutoTokenizer.from_pretrained(model_id)\n",
    "model_2b = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    token=token,\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Adapter\n",
    "Caricamento dell'adattatore LoRA precedentemente fine-tunato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:08:32.714682Z",
     "iopub.status.busy": "2025-06-02T18:08:32.714067Z",
     "iopub.status.idle": "2025-06-02T18:08:34.415681Z",
     "shell.execute_reply": "2025-06-02T18:08:34.415077Z",
     "shell.execute_reply.started": "2025-06-02T18:08:32.714657Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e67e16b8f04fbf937cb69d127fb43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/800 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a319ba36c0d546aeb2d32512f7d73dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/41.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_2b.load_adapter(\"stefra/GEMMA2BITMATHR8A16\", adapter_name=\"default\")\n",
    "#model_2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:08:35.575573Z",
     "iopub.status.busy": "2025-06-02T18:08:35.575238Z",
     "iopub.status.idle": "2025-06-02T18:08:35.581692Z",
     "shell.execute_reply": "2025-06-02T18:08:35.580983Z",
     "shell.execute_reply.started": "2025-06-02T18:08:35.575553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['default']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2b.active_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:08:37.314461Z",
     "iopub.status.busy": "2025-06-02T18:08:37.313750Z",
     "iopub.status.idle": "2025-06-02T18:08:37.326386Z",
     "shell.execute_reply": "2025-06-02T18:08:37.325677Z",
     "shell.execute_reply.started": "2025-06-02T18:08:37.314437Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_2b.set_adapter(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova per verificare che l'adattatore sia effettivamente attivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:08:43.544072Z",
     "iopub.status.busy": "2025-06-02T18:08:43.543372Z",
     "iopub.status.idle": "2025-06-02T18:09:08.028285Z",
     "shell.execute_reply": "2025-06-02T18:09:08.027573Z",
     "shell.execute_reply.started": "2025-06-02T18:08:43.544051Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have that\n",
      "\\[\\mathbf{a} \\cdot \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = x + y + z.\\]Also,\n",
      "\\[\\mathbf{a} \\cdot \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ -y \\\\ z \\end{pmatrix} = x - y + z.\\]Thus, $x + y + z = 2$ and $x - y + z = 2.$  Adding these equations, we get $2x = 4,$ so $x = 2.$  Then $y = -2,$ and $z = 1.$  Therefore, $\\mathbf{v} = \\boxed{\\begin{pmatrix} 2 \\\\ -2 \\\\ 1 \\end{pmatrix}}.$\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_2b(chat_input, return_tensors=\"pt\").to(model_2b.device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = model_2b.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024\n",
    "    )\n",
    "full_output = tokenizer_2b.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "input_length = len(tokenizer_2b.decode(inputs.input_ids[0], skip_special_tokens=True))\n",
    "generated_text = full_output[input_length:].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the weights\n",
    "Qui ci andiamo a definire due dizionari nella quale ci andiamo a recuperare i pesi delle varie matrici, sia modello piccolo che modello grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:09:51.131125Z",
     "iopub.status.busy": "2025-06-02T18:09:51.130850Z",
     "iopub.status.idle": "2025-06-02T18:09:51.149633Z",
     "shell.execute_reply": "2025-06-02T18:09:51.149113Z",
     "shell.execute_reply.started": "2025-06-02T18:09:51.131106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Dizionario di pesi LoRA dal modello 2B\n",
    "lora_weights_2b = {}\n",
    "for name, param in model_2b.model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        lora_weights_2b[name] = param.data\n",
    "\n",
    "# Dizionario di pesi LoRA dal modello 9B\n",
    "lora_weights_model_9 = {}\n",
    "for name, param in peft_model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        lora_weights_model_9[name] = param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-06-02T18:09:58.572344Z",
     "iopub.status.busy": "2025-06-02T18:09:58.572049Z",
     "iopub.status.idle": "2025-06-02T18:10:24.688876Z",
     "shell.execute_reply": "2025-06-02T18:10:24.688144Z",
     "shell.execute_reply.started": "2025-06-02T18:09:58.572326Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espansione smart per layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.0.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.0.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.0.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.0.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.0.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.0.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.0.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.0.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.0.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.0.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.0.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.0.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.1.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.1.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.1.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.1.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.1.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.1.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.1.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.1.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.1.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.1.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.1.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.1.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.1.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.1.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.2.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.2.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.2.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.2.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.2.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.2.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.2.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.2.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.2.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.2.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.2.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.2.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.2.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.2.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.3.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.3.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.3.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.3.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.3.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.3.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.3.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.3.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.3.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.3.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.3.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.3.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.3.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.3.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.4.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.4.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.4.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.4.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.4.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.4.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.4.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.4.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.4.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.4.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.4.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.4.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.4.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.4.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.5.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.5.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.5.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.5.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.5.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.5.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.5.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.5.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.5.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.5.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.5.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.5.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.5.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.5.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.6.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.6.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.6.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.6.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.6.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.6.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.6.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.6.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.6.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.6.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.6.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.6.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.6.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.6.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.7.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.7.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.7.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.7.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.7.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.7.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.7.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.7.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.7.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.7.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.7.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.7.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.7.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.7.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.8.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.8.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.8.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.8.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.8.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.8.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.8.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.8.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.8.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.8.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.8.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.8.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.8.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.8.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.9.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.9.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.9.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.9.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.9.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.9.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.9.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.9.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.9.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.9.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.9.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.9.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.9.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.9.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.10.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.10.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.10.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.10.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.10.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.10.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.10.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.10.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.10.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.10.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.10.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.10.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.10.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.10.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.11.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.11.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.11.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.11.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.11.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.11.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.11.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.11.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.11.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.11.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.11.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.11.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.11.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.11.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.12.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.12.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.12.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.12.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.12.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.12.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.12.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.12.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.12.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.12.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.12.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.12.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.12.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.12.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.13.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.13.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.13.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.13.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.13.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.13.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.13.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.13.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.13.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.13.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.13.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.13.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.13.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.13.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.14.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.14.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.14.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.14.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.14.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.14.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.14.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.14.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.14.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.14.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.14.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.14.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.14.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.14.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.15.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.15.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.15.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.15.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.15.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.15.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.15.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.15.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.15.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.15.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.15.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.15.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.15.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.15.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.16.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.16.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.16.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.16.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.16.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.16.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.16.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.16.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.16.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.16.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.16.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.16.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.16.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.16.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.17.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.17.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.17.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.17.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.17.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.17.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.17.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.17.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.17.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.17.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.17.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.17.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.17.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.17.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.18.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.18.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.18.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.18.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.18.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.18.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.18.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.18.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.18.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.18.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.18.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.18.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.18.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.18.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.19.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.19.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.19.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.19.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.19.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.19.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.19.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.19.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.19.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.19.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.19.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.19.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.19.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.19.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.20.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.20.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.20.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.20.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.20.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.20.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.20.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.20.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.20.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.20.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.20.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.20.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.20.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.20.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.21.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.21.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.21.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.21.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.21.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.21.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.21.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.21.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.21.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.21.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.21.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.21.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.21.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.21.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.22.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.22.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.22.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.22.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.22.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.22.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.22.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.22.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.22.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.22.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.22.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.22.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.22.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.22.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.23.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.23.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.23.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.23.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.23.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.23.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.23.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.23.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.23.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.23.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.23.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.23.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.23.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.23.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.24.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.24.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.24.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.24.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.24.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.24.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.24.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.24.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.24.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.24.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.24.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.24.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.24.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.24.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.25.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.25.self_attn.q_proj.lora_B.default.weight: torch.Size([2048, 8]) -> torch.Size([4096, 8])\n",
      "Espansione smart per layers.25.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.25.self_attn.k_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.25.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.25.self_attn.v_proj.lora_B.default.weight: torch.Size([1024, 8]) -> torch.Size([2048, 8])\n",
      "Espansione smart per layers.25.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 2048]) -> torch.Size([8, 4096])\n",
      "Espansione smart per layers.25.self_attn.o_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Espansione smart per layers.25.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.25.mlp.gate_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.25.mlp.up_proj.lora_A.default.weight: torch.Size([8, 2304]) -> torch.Size([8, 3584])\n",
      "Espansione smart per layers.25.mlp.up_proj.lora_B.default.weight: torch.Size([9216, 8]) -> torch.Size([14336, 8])\n",
      "Espansione smart per layers.25.mlp.down_proj.lora_A.default.weight: torch.Size([8, 9216]) -> torch.Size([8, 14336])\n",
      "Espansione smart per layers.25.mlp.down_proj.lora_B.default.weight: torch.Size([2304, 8]) -> torch.Size([3584, 8])\n",
      "Layer base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "Layer base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight non trovato nel modello 2B. Inizializzazione Xavier.\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.40.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.40.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.40.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.40.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.40.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.40.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.40.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.41.self_attn.q_proj.lora_B.default.weight: torch.Size([4096, 8])\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.41.self_attn.k_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.41.self_attn.v_proj.lora_B.default.weight: torch.Size([2048, 8])\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_A.default.weight: torch.Size([8, 4096])\n",
      "base_model.model.model.layers.41.self_attn.o_proj.lora_B.default.weight: torch.Size([3584, 8])\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.41.mlp.gate_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_A.default.weight: torch.Size([8, 3584])\n",
      "base_model.model.model.layers.41.mlp.up_proj.lora_B.default.weight: torch.Size([14336, 8])\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_A.default.weight: torch.Size([8, 14336])\n",
      "base_model.model.model.layers.41.mlp.down_proj.lora_B.default.weight: torch.Size([3584, 8])\n"
     ]
    }
   ],
   "source": [
    "def xavier_initialization(tensor):\n",
    "    \"\"\"Inizializza i pesi con la distribuzione Xavier\"\"\"\n",
    "    return nn.init.xavier_uniform_(tensor)\n",
    "\n",
    "def smart_lora_expansion(weight_2b, target_shape):\n",
    "    \"\"\"\n",
    "    Espande secondo quanto riportato nella relazione, i pesi LoRA preservando pattern semantici.\n",
    "    \"\"\"\n",
    "    current_shape = weight_2b.shape # dimensione matrice di partenza\n",
    "    scale_factor = 0.5 #scaling factor per preservare i pattern semantici\n",
    "\n",
    "    # Se le forme sono già uguali, restituisci il peso originale senza andare ad espandere\n",
    "    if current_shape == target_shape:\n",
    "        return weight_2b.clone()\n",
    "    \n",
    "    # Altrimenti creiamo un tensore espanso con la forma target, inizializzato a zero\n",
    "    expanded = torch.zeros(target_shape, dtype=weight_2b.dtype, device=weight_2b.device)\n",
    "    \n",
    "    if len(current_shape) == 2:  # Matrice 2D (caso più comune per LoRA)\n",
    "        old_rows, old_cols = current_shape\n",
    "        new_rows, new_cols = target_shape\n",
    "        \n",
    "        # Copia i pesi originali nella porzione corrispondente\n",
    "        copy_rows = min(old_rows, new_rows)\n",
    "        copy_cols = min(old_cols, new_cols)\n",
    "        expanded[:copy_rows, :copy_cols] = weight_2b[:copy_rows, :copy_cols]\n",
    "        \n",
    "        ################ FASE DI ESPANSIONE ################\n",
    "        # righe\n",
    "        if new_rows > old_rows:\n",
    "            for i in range(old_rows, new_rows):\n",
    "                source_row = i % old_rows  # Cicla attraverso le righe esistenti\n",
    "                expanded[i, :copy_cols] = weight_2b[source_row, :copy_cols] * scale_factor\n",
    "        \n",
    "        # colonne\n",
    "        if new_cols > old_cols:\n",
    "            for j in range(old_cols, new_cols):\n",
    "                source_col = j % old_cols  # Cicla attraverso le colonne esistenti\n",
    "                # Applica a tutte le righe (sia originali che espanse)\n",
    "                expanded[:, j] = expanded[:, source_col] * scale_factor # qui è dove avviene questa espansione \"intelligente\"\n",
    "                \n",
    "    elif len(current_shape) == 1:  # Vettore 1D (bias)\n",
    "        old_size = current_shape[0]\n",
    "        new_size = target_shape[0]\n",
    "        \n",
    "        # Copia i valori originali\n",
    "        copy_size = min(old_size, new_size)\n",
    "        expanded[:copy_size] = weight_2b[:copy_size]\n",
    "        \n",
    "        # Espansione\n",
    "        if new_size > old_size:\n",
    "            for i in range(old_size, new_size):\n",
    "                source_idx = i % old_size\n",
    "                expanded[i] = weight_2b[source_idx] * scale_factor\n",
    "                \n",
    "    else:\n",
    "        # Per tensori con più di 2 dimensioni si usa Xavier come fallback\n",
    "        print(f\"Dimensioni non supportate per smart expansion: {current_shape}. Uso Xavier.\")\n",
    "        return xavier_initialization(expanded)\n",
    "    \n",
    "    return expanded\n",
    "\n",
    "def new_lora_weights_smart(lora_weights_2b, lora_weights_9b):\n",
    "    \"\"\"\n",
    "    Funzione di richiamo per l'espansione delle matrici LoRA\n",
    "    \"\"\"\n",
    "\n",
    "    new_weights = {}\n",
    "    \n",
    "    for name, weight_9b in lora_weights_9b.items():\n",
    "        # Rimozione della parte 'base_model.model.model.' dal nome della chiave per adattarlo a quello del modello 2B\n",
    "        name_2b = name.replace('base_model.model.model.', '')\n",
    "        \n",
    "        if name_2b in lora_weights_2b:\n",
    "            weight_2b = lora_weights_2b[name_2b]\n",
    "            \n",
    "            # Verifica compatibilità delle dimensioni\n",
    "            if weight_9b.dim() != weight_2b.dim():\n",
    "                print(f\"ATTENZIONE: Dimensioni incompatibili per {name_2b}! \"\n",
    "                      f\"2B: {weight_2b.dim()}D, 9B: {weight_9b.dim()}D\")\n",
    "\n",
    "                new_weights[name] = xavier_initialization(torch.zeros_like(weight_9b))\n",
    "                continue\n",
    "            \n",
    "            ############## RICHIAMO FUNZIONE SMART ESPANSIONE ##############\n",
    "            if weight_9b.size() != weight_2b.size():\n",
    "                print(f\"Espansione smart per {name_2b}: {weight_2b.size()} -> {weight_9b.size()}\")\n",
    "                new_weights[name] = smart_lora_expansion(weight_2b, weight_9b.size())\n",
    "            else:\n",
    "                # Le dimensioni sono uguali quindi copia direttamente\n",
    "                new_weights[name] = weight_2b.clone()\n",
    "                \n",
    "        else:\n",
    "            # Se non ci sono pesi corrispondenti in 2B, inizializza con Xavier\n",
    "            print(f\"Layer {name} non trovato nel modello 2B. Inizializzazione Xavier.\")\n",
    "            new_weights[name] = xavier_initialization(torch.zeros_like(weight_9b))\n",
    "    \n",
    "    return new_weights\n",
    "\n",
    "new_lora_weights_big = new_lora_weights_smart(lora_weights_2b, lora_weights_model_9)\n",
    "\n",
    "# Per verificare che tutto sia andato bene, stampiamo i nomi dei pesi interpolati\n",
    "for name, weight in new_lora_weights_big.items():\n",
    "    print(f'{name}: {weight.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:15.821896Z",
     "iopub.status.busy": "2025-06-02T18:11:15.821603Z",
     "iopub.status.idle": "2025-06-02T18:11:15.826879Z",
     "shell.execute_reply": "2025-06-02T18:11:15.826203Z",
     "shell.execute_reply.started": "2025-06-02T18:11:15.821879Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_lora_weights_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:17.096667Z",
     "iopub.status.busy": "2025-06-02T18:11:17.096359Z",
     "iopub.status.idle": "2025-06-02T18:11:17.101512Z",
     "shell.execute_reply": "2025-06-02T18:11:17.100776Z",
     "shell.execute_reply.started": "2025-06-02T18:11:17.096630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lora_weights_model_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:18.767494Z",
     "iopub.status.busy": "2025-06-02T18:11:18.766724Z",
     "iopub.status.idle": "2025-06-02T18:11:18.772890Z",
     "shell.execute_reply": "2025-06-02T18:11:18.772308Z",
     "shell.execute_reply.started": "2025-06-02T18:11:18.767470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:1')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(new_lora_weights_big['base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:20.347858Z",
     "iopub.status.busy": "2025-06-02T18:11:20.347141Z",
     "iopub.status.idle": "2025-06-02T18:11:20.352937Z",
     "shell.execute_reply": "2025-06-02T18:11:20.352401Z",
     "shell.execute_reply.started": "2025-06-02T18:11:20.347832Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(lora_weights_2b['layers.0.self_attn.q_proj.lora_B.default.weight']==0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifica se ci sia una matrice inizializzata tutta pari a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:21.841713Z",
     "iopub.status.busy": "2025-06-02T18:11:21.841397Z",
     "iopub.status.idle": "2025-06-02T18:11:21.882665Z",
     "shell.execute_reply": "2025-06-02T18:11:21.881970Z",
     "shell.execute_reply.started": "2025-06-02T18:11:21.841666Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0 \n",
    "for name, weight in new_lora_weights_big.items():\n",
    "    if torch.all(weight== 0):\n",
    "        i = i+1\n",
    "i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update weights\n",
    "Fase di aggiornamento dei pesi della matrice LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:23.930090Z",
     "iopub.status.busy": "2025-06-02T18:11:23.929331Z",
     "iopub.status.idle": "2025-06-02T18:11:23.938435Z",
     "shell.execute_reply": "2025-06-02T18:11:23.937600Z",
     "shell.execute_reply.started": "2025-06-02T18:11:23.930058Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0039, -0.0129,  0.0111,  ..., -0.0022, -0.0006,  0.0105],\n",
       "        [ 0.0050,  0.0070,  0.0046,  ..., -0.0153, -0.0092,  0.0138],\n",
       "        [-0.0098, -0.0110,  0.0113,  ...,  0.0046,  0.0050, -0.0089],\n",
       "        ...,\n",
       "        [ 0.0089, -0.0066, -0.0130,  ...,  0.0029, -0.0152, -0.0081],\n",
       "        [-0.0013,  0.0052, -0.0122,  ..., -0.0082,  0.0099, -0.0146],\n",
       "        [ 0.0022,  0.0010,  0.0141,  ...,  0.0130,  0.0156,  0.0160]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight #matrice di partenza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:26.691955Z",
     "iopub.status.busy": "2025-06-02T18:11:26.691671Z",
     "iopub.status.idle": "2025-06-02T18:11:26.749113Z",
     "shell.execute_reply": "2025-06-02T18:11:26.748362Z",
     "shell.execute_reply.started": "2025-06-02T18:11:26.691936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#AGGIORNAMENTO PESI LORA\n",
    "base_model_state_dict = peft_model.state_dict()\n",
    "\n",
    "for name, interpolated_weight in new_lora_weights_big.items():\n",
    "    if name in base_model_state_dict:\n",
    "        base_model_state_dict[name].data.copy_(interpolated_weight)\n",
    "    else:\n",
    "        print(f\"Peso {name} non trovato nel modello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:11:29.035245Z",
     "iopub.status.busy": "2025-06-02T18:11:29.034984Z",
     "iopub.status.idle": "2025-06-02T18:11:29.044015Z",
     "shell.execute_reply": "2025-06-02T18:11:29.043314Z",
     "shell.execute_reply.started": "2025-06-02T18:11:29.035226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0044,  0.0036,  0.0053,  ...,  0.0166,  0.0129,  0.0003],\n",
       "        [-0.0168,  0.0066,  0.0210,  ...,  0.0094,  0.0250,  0.0120],\n",
       "        [-0.0114,  0.0111,  0.0274,  ..., -0.0061,  0.0158, -0.0054],\n",
       "        ...,\n",
       "        [-0.0011, -0.0093,  0.0021,  ..., -0.0067,  0.0018, -0.0126],\n",
       "        [ 0.0122,  0.0073, -0.0139,  ...,  0.0197, -0.0093, -0.0103],\n",
       "        [ 0.0183, -0.0182, -0.0130,  ...,  0.0133, -0.0081,  0.0127]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model.base_model.model.model.layers[0].self_attn.q_proj.lora_A.default.weight #matrice di partenza aggiornata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "**Understanding the Problem**\n",
      "\n",
      "* **Dot Product:** The dot product of two vectors gives a scalar (a number).  The equation $\\mathbf{a} \\cdot \\mathbf{v} = 2$ tells us the result of the dot product of  $\\mathbf{a}$ and $\\mathbf{v}$ is 2.\n",
      "* **Cross Product:** The cross product of two vectors results in a vector. The equation $\\mathbf{a} \\times \\mathbf{v} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$ tells us the cross product of $\\mathbf{a}$ and $\\mathbf{v}$ is the vector $\\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n",
      "\n",
      "**Solution**\n",
      "\n",
      "1. **Dot Product:**\n",
      "   Let $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$.  The dot product is:\n",
      "   \n",
      "   $$\\mathbf{a} \\cdot \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = x + y + z = 2$$\n",
      "\n",
      "2. **Cross Product:**\n",
      "   The cross product of two vectors is calculated as follows:\n",
      "\n",
      "   $$\\mathbf{a} \\times \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\times \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} (1)(z) - (1)(y) \\\\ (1)(x) - (1)(z) \\\\ (1)(y) - (1)(x) \\end{pmatrix} = \\begin{pmatrix} z - y \\\\ x - z \\\\ y - x \\end{pmatrix}$$\n",
      "\n",
      "   We are given that this cross product is equal to $\\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$.  This gives us the system of equations:\n",
      "\n",
      "   *  $z - y = 1$\n",
      "   *  $x - z = -2$\n",
      "   *  $y - x = 1$\n",
      "\n",
      "3. **Solving the System of Equations:**\n",
      "   We can solve this system of equations using various methods (substitution, elimination, etc.).  Solving, we find:\n",
      "   *  $x = -1$\n",
      "   *  $y = 0$\n",
      "   *  $z = 1$\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Therefore, the vector $\\mathbf{v}$ is:\n",
      "\n",
      "$$\\mathbf{v} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model_9b.device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = model_9b.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "    )\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "input_length = len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True))\n",
    "generated_text = full_output[input_length:].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:15:07.606386Z",
     "iopub.status.busy": "2025-06-02T18:15:07.606090Z",
     "iopub.status.idle": "2025-06-02T18:15:14.676539Z",
     "shell.execute_reply": "2025-06-02T18:15:14.675767Z",
     "shell.execute_reply.started": "2025-06-02T18:15:07.606366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a408560f0354d9bb781f6f609b74bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/108M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/stefra/math_scaling_1/commit/740f7ebdde4208a2a2c2e4ea93eb2541c2520835', commit_message='Upload LoRA adapter', commit_description='', oid='740f7ebdde4208a2a2c2e4ea93eb2541c2520835', pr_url=None, repo_url=RepoUrl('https://huggingface.co/stefra/math_scaling_1', endpoint='https://huggingface.co', repo_type='model', repo_id='stefra/math_scaling_1'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvataggio locale e su HF\n",
    "peft_model.save_pretrained(\"./math_scaling_1\")\n",
    "peft_model.push_to_hub(\"stefra/math_scaling_1\", \n",
    "                       commit_message=\"Upload LoRA adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing v2 w loading from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:15:27.254890Z",
     "iopub.status.busy": "2025-06-02T18:15:27.254174Z",
     "iopub.status.idle": "2025-06-02T18:15:30.140693Z",
     "shell.execute_reply": "2025-06-02T18:15:30.140133Z",
     "shell.execute_reply.started": "2025-06-02T18:15:27.254867Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "114e6cb4ae4045f496625746a732e8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/896 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45e71503c0e45328510cd81389964ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/108M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_9b.load_adapter(\"stefra/math_scaling_1\", adapter_name=\"default\")\n",
    "#model_2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:15:31.318840Z",
     "iopub.status.busy": "2025-06-02T18:15:31.318562Z",
     "iopub.status.idle": "2025-06-02T18:15:31.335234Z",
     "shell.execute_reply": "2025-06-02T18:15:31.334451Z",
     "shell.execute_reply.started": "2025-06-02T18:15:31.318820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_9b.set_adapter(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve the problem:\n",
      "\n",
      "**Understanding the Problem**\n",
      "\n",
      "* **Dot Product:** The dot product of two vectors gives a scalar (a number).  The equation $\\mathbf{a} \\cdot \\mathbf{v} = 2$ tells us the result of the dot product of  $\\mathbf{a}$ and $\\mathbf{v}$ is 2.\n",
      "* **Cross Product:** The cross product of two vectors results in a vector. The equation $\\mathbf{a} \\times \\mathbf{v} = \\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$ tells us the cross product of $\\mathbf{a}$ and $\\mathbf{v}$ is the vector $\\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$.\n",
      "\n",
      "**Solution**\n",
      "\n",
      "1. **Dot Product:**\n",
      "   Let $\\mathbf{v} = \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix}$.  The dot product is:\n",
      "   \n",
      "   $$\\mathbf{a} \\cdot \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = x + y + z = 2$$\n",
      "\n",
      "2. **Cross Product:**\n",
      "   The cross product of two vectors is calculated as follows:\n",
      "\n",
      "   $$\\mathbf{a} \\times \\mathbf{v} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\times \\begin{pmatrix} x \\\\ y \\\\ z \\end{pmatrix} = \\begin{pmatrix} (1)(z) - (1)(y) \\\\ (1)(x) - (1)(z) \\\\ (1)(y) - (1)(x) \\end{pmatrix} = \\begin{pmatrix} z - y \\\\ x - z \\\\ y - x \\end{pmatrix}$$\n",
      "\n",
      "   We are given that this cross product is equal to $\\begin{pmatrix} 1 \\\\ -2 \\\\ 1 \\end{pmatrix}$.  This gives us the system of equations:\n",
      "\n",
      "   *  $z - y = 1$\n",
      "   *  $x - z = -2$\n",
      "   *  $y - x = 1$\n",
      "\n",
      "3. **Solving the System of Equations:**\n",
      "   We can solve this system of equations using various methods (substitution, elimination, etc.).  Solving, we find:\n",
      "   *  $x = -1$\n",
      "   *  $y = 0$\n",
      "   *  $z = 1$\n",
      "\n",
      "**Answer:**\n",
      "\n",
      "Therefore, the vector $\\mathbf{v}$ is:\n",
      "\n",
      "$$\\mathbf{v} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(chat_input, return_tensors=\"pt\").to(model_9b.device)\n",
    "    \n",
    "with torch.no_grad():\n",
    "    outputs = model_9b.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "    )\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "input_length = len(tokenizer.decode(inputs.input_ids[0], skip_special_tokens=True))\n",
    "generated_text = full_output[input_length:].strip()\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
