{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caratteristiche del notebook\n",
    "\n",
    "- System_prompt presente = No\n",
    "- Dataset usato = Hendrycks/math\n",
    "- Epoche = 3\n",
    "- Fine Tuning eseguito = Si - Repo -> francescoocurcio/new_llama3.2-3B-math-ftn-math-3epoch_12.5k-sysprompt_no\n",
    "- Addestrato solo sulle risposte = No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importazione delle librerie e definizione delle costanti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:03:43.404700Z",
     "iopub.status.busy": "2025-05-10T11:03:43.404044Z",
     "iopub.status.idle": "2025-05-10T11:06:55.000037Z",
     "shell.execute_reply": "2025-05-10T11:06:54.999149Z",
     "shell.execute_reply.started": "2025-05-10T11:03:43.404677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pip3-autoremove\n",
    "!pip-autoremove torch torchvision torchaudio -y\n",
    "!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:06:59.850925Z",
     "iopub.status.busy": "2025-05-10T11:06:59.850158Z",
     "iopub.status.idle": "2025-05-10T11:07:26.773145Z",
     "shell.execute_reply": "2025-05-10T11:07:26.772580Z",
     "shell.execute_reply.started": "2025-05-10T11:06:59.850897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "# IMPORT DELLE LIBRERIE\n",
    "#########################\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)  # Mostra tutte le colonne\n",
    "pd.set_option('display.width', None)        # Non tronca l'output a una larghezza fissa\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "from unsloth import FastLanguageModel, to_sharegpt\n",
    "from unsloth.chat_templates import get_chat_template, standardize_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "#########################\n",
    "# COSTANTI\n",
    "#########################\n",
    "\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = True\n",
    "\n",
    "OUTPUT_REPO = \"francescoocurcio/new_llama3.2-3B-math-ftn-math-3epoch_12.5k-sysprompt_no\"\n",
    "SAVE_DIRECTORY = \"/kaggle/working/new_llama3.2-3B-math-ftn-math-3epoch_12.5k-sysprompt_no\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:07:29.314987Z",
     "iopub.status.busy": "2025-05-10T11:07:29.314700Z",
     "iopub.status.idle": "2025-05-10T11:07:30.003298Z",
     "shell.execute_reply": "2025-05-10T11:07:30.002523Z",
     "shell.execute_reply.started": "2025-05-10T11:07:29.314967Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login --token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selezione e configurazione del modello: Llama3.2 3B Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:07:37.999274Z",
     "iopub.status.busy": "2025-05-10T11:07:37.995909Z",
     "iopub.status.idle": "2025-05-10T11:08:01.968737Z",
     "shell.execute_reply": "2025-05-10T11:08:01.968114Z",
     "shell.execute_reply.started": "2025-05-10T11:07:37.999236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT\n",
    "    # token = \"hf...\" #Use one if using gated models like meta-llama/Llama....\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "#PEFT = Parameter Efficient Fine Tuning\n",
    "model = FastLanguageModel.get_peft_model( #Modello quantizzato\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 42,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:12:57.734558Z",
     "iopub.status.busy": "2025-05-10T10:12:57.733902Z",
     "iopub.status.idle": "2025-05-10T10:13:40.060824Z",
     "shell.execute_reply": "2025-05-10T10:13:40.060225Z",
     "shell.execute_reply.started": "2025-05-10T10:12:57.734534Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET = \"EleutherAI/hendrycks_math\"\n",
    "print(\"Caricamento del dataset matematico...\")\n",
    "\n",
    "algebra_train = load_dataset(DATASET, 'algebra')['train']\n",
    "algebra_test = load_dataset(DATASET, 'algebra')['test']\n",
    "\n",
    "counting_and_probability_train = load_dataset(DATASET, 'counting_and_probability')['train']\n",
    "counting_and_probability_test = load_dataset(DATASET, 'counting_and_probability')['test']\n",
    "\n",
    "geometry_train = load_dataset(DATASET, 'geometry')['train']\n",
    "geometry_test = load_dataset(DATASET, 'geometry')['test']\n",
    "\n",
    "number_theory_train = load_dataset(DATASET, 'number_theory')['train']\n",
    "number_theory_test = load_dataset(DATASET, 'number_theory')['test']\n",
    "\n",
    "intermediate_algebra_train = load_dataset(DATASET, 'intermediate_algebra')['train']\n",
    "intermediate_algebra_test = load_dataset(DATASET, 'intermediate_algebra')['test']\n",
    "\n",
    "prealgebra_train = load_dataset(DATASET, 'prealgebra')['train']\n",
    "prealgebra_test = load_dataset(DATASET, 'prealgebra')['test']\n",
    "\n",
    "precalculus_train = load_dataset(DATASET, 'precalculus')['train']\n",
    "precalculus_test = load_dataset(DATASET, 'precalculus')['test']\n",
    "\n",
    "algebra_train_df = pd.DataFrame(algebra_train)\n",
    "algebra_test_df = pd.DataFrame(algebra_test)\n",
    "\n",
    "counting_and_probability_train_df = pd.DataFrame(counting_and_probability_train)\n",
    "counting_and_probability_test_df = pd.DataFrame(counting_and_probability_test)\n",
    "\n",
    "geometry_train_df = pd.DataFrame(geometry_train)\n",
    "geometry_test_df = pd.DataFrame(geometry_test)\n",
    "\n",
    "number_theory_train_df = pd.DataFrame(number_theory_train)\n",
    "number_theory_test_df = pd.DataFrame(number_theory_test)\n",
    "\n",
    "intermediate_algebra_train_df = pd.DataFrame(intermediate_algebra_train)\n",
    "intermediate_algebra_test_df = pd.DataFrame(intermediate_algebra_test)\n",
    "\n",
    "prealgebra_train_df = pd.DataFrame(prealgebra_train)\n",
    "prealgebra_test_df = pd.DataFrame(prealgebra_test)\n",
    "\n",
    "precalculus_train_df = pd.DataFrame(precalculus_train)\n",
    "precalculus_test_df = pd.DataFrame(precalculus_test)\n",
    "\n",
    "dataset_train_df = pd.concat([algebra_train_df, counting_and_probability_train_df, geometry_train_df,\n",
    "                              number_theory_train_df, intermediate_algebra_train_df,\n",
    "                              prealgebra_train_df, precalculus_train_df], ignore_index=True)\n",
    "\n",
    "dataset_test_df = pd.concat([algebra_test_df, counting_and_probability_test_df, geometry_test_df,\n",
    "                             number_theory_test_df, intermediate_algebra_test_df,\n",
    "                             prealgebra_test_df, precalculus_test_df], ignore_index=True)\n",
    "\n",
    "dataset_math_final = pd.concat([dataset_train_df, dataset_test_df], ignore_index=True)\n",
    "\n",
    "print(\"Dataset concatenato (train + test):\")\n",
    "dataset_math_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:14:52.650496Z",
     "iopub.status.busy": "2025-05-10T10:14:52.649887Z",
     "iopub.status.idle": "2025-05-10T10:14:52.924989Z",
     "shell.execute_reply": "2025-05-10T10:14:52.924190Z",
     "shell.execute_reply.started": "2025-05-10T10:14:52.650473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import to_sharegpt\n",
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_pandas(dataset_math_final)\n",
    "\n",
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt = \"Solve the following problem.[[:\\n{problem}]]\",\n",
    "    output_column_name = \"solution\",\n",
    "    conversation_extension = 1, #Select more to handle longer conversations\n",
    ")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opzione di aggiunta per il system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#system_message = {\n",
    "#    \"from\": \"system\",\n",
    "#    \"value\": (\n",
    "#        \"You are a math expert. You will be given a mathematical problem to solve. \"\n",
    "#        \"Your aim is to first provide a step-by-step explanation of the solution \"\n",
    "#        \"(integrating the formulae in LaTeX syntax) and then to conclude with a clear and concise final answer.\"\n",
    "#    )\n",
    "#}\n",
    "\n",
    "# Aggiungilo all'inizio di ogni conversazione\n",
    "#def add_system_message(example):\n",
    "#    conversation = example[\"conversations\"]\n",
    "#    return {\n",
    "#        \"conversations\": [system_message] + conversation\n",
    "#    }\n",
    "\n",
    "# Applica la funzione a tutto il dataset\n",
    "#dataset = dataset.map(add_system_message)\n",
    "#print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Costruzione dataset finale conversazionale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:17:42.911662Z",
     "iopub.status.busy": "2025-05-10T10:17:42.911333Z",
     "iopub.status.idle": "2025-05-10T10:17:43.820901Z",
     "shell.execute_reply": "2025-05-10T10:17:43.820150Z",
     "shell.execute_reply.started": "2025-05-10T10:17:42.911640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:17:55.200875Z",
     "iopub.status.busy": "2025-05-10T10:17:55.200140Z",
     "iopub.status.idle": "2025-05-10T10:17:56.315584Z",
     "shell.execute_reply": "2025-05-10T10:17:56.314909Z",
     "shell.execute_reply.started": "2025-05-10T10:17:55.200843Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:21:05.944325Z",
     "iopub.status.busy": "2025-05-10T10:21:05.944055Z",
     "iopub.status.idle": "2025-05-10T10:21:05.949252Z",
     "shell.execute_reply": "2025-05-10T10:21:05.948479Z",
     "shell.execute_reply.started": "2025-05-10T10:21:05.944304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)\n",
    "#Anche dopo aver cambiato inizialmente queste operazioni il tokenizer a fronte delle\n",
    "#operazioni eseguite è come se eseguisse una sorta di riformattazione\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "print(tokenizer.pad_token)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.padding_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:21:23.527426Z",
     "iopub.status.busy": "2025-05-10T10:21:23.526936Z",
     "iopub.status.idle": "2025-05-10T10:21:23.539201Z",
     "shell.execute_reply": "2025-05-10T10:21:23.538631Z",
     "shell.execute_reply.started": "2025-05-10T10:21:23.527407Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text = dataset[0]['text']\n",
    "tokenized = tokenizer(text, return_tensors=\"pt\", return_attention_mask=True)\n",
    "input_ids = tokenized[\"input_ids\"][0]\n",
    "\n",
    "decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for i, (token_id, token_str) in enumerate(zip(input_ids, decoded_tokens)):\n",
    "    print(f\"{i:03d} | Token ID: {token_id.item():>6} | Token: {repr(token_str)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestramento del modello tramite LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:36:17.439419Z",
     "iopub.status.busy": "2025-05-10T10:36:17.439065Z",
     "iopub.status.idle": "2025-05-10T10:36:19.564267Z",
     "shell.execute_reply": "2025-05-10T10:36:19.563626Z",
     "shell.execute_reply.started": "2025-05-10T10:36:17.439396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs and \"loss\" in logs:\n",
    "            loss = logs[\"loss\"]\n",
    "            step = state.global_step\n",
    "            self.train_losses.append((step, loss))\n",
    "            print(f\"📉 Step {step} - Loss: {loss:.4f}\")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "loss_callback = LossLoggerCallback()\n",
    "\n",
    "#model.config.use_cache = False\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    do_train                    = True,\n",
    "\n",
    "    dataset_text_field          = \"text\",\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 8,\n",
    "\n",
    "    num_train_epochs            = 3,   # Epoche complete\n",
    "    #max_steps                   = 10,\n",
    "    \n",
    "    learning_rate               = 2e-4,\n",
    "    lr_scheduler_type           = \"linear\",\n",
    "    logging_strategy            = 'steps',\n",
    "    logging_steps               = 20,\n",
    "    # 💾 Salvataggio\n",
    "    save_strategy               = 'epoch',\n",
    "    #save_steps                  = 200,\n",
    "\n",
    "    warmup_steps                = 40,\n",
    "\n",
    "    optim                       = \"adamw_8bit\",\n",
    "    seed                        = 42,\n",
    "\n",
    "    fp16                        = not is_bfloat16_supported(),\n",
    "    bf16                        = is_bfloat16_supported(),\n",
    "\n",
    "    weight_decay                = 0.01,\n",
    "    report_to                   = \"none\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model              = model,\n",
    "    tokenizer          = tokenizer,\n",
    "    dataset_num_proc   = 2,\n",
    "    max_seq_length     = MAX_SEQ_LENGTH,\n",
    "    train_dataset      = dataset,\n",
    "    args               = training_args,\n",
    "    data_collator      = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing            = False,\n",
    "    callbacks          = [loss_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:36:22.221205Z",
     "iopub.status.busy": "2025-05-10T10:36:22.220477Z",
     "iopub.status.idle": "2025-05-10T10:49:40.602400Z",
     "shell.execute_reply": "2025-05-10T10:49:40.601831Z",
     "shell.execute_reply.started": "2025-05-10T10:36:22.221173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"Avvio addestramento LoRA...\")\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Salvataggio del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:11:15.439402Z",
     "iopub.status.busy": "2025-05-10T11:11:15.438555Z",
     "iopub.status.idle": "2025-05-10T11:11:15.622752Z",
     "shell.execute_reply": "2025-05-10T11:11:15.621776Z",
     "shell.execute_reply.started": "2025-05-10T11:11:15.439369Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir $SAVE_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T11:11:17.127724Z",
     "iopub.status.busy": "2025-05-10T11:11:17.126863Z",
     "iopub.status.idle": "2025-05-10T11:11:28.999916Z",
     "shell.execute_reply": "2025-05-10T11:11:28.999076Z",
     "shell.execute_reply.started": "2025-05-10T11:11:17.127687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# PUSH MODELLO LoRA + TOKENIZER su HUGGING FACE\n",
    "print(\"🔄 Caricamento del modello e del tokenizer in corso...\")\n",
    "model.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "tokenizer.push_to_hub(OUTPUT_REPO, token=HF_UNIVERSAL_TOKEN, private=True)\n",
    "\n",
    "print(f\"✅ Modello caricato correttamente su: {OUTPUT_REPO}\\n\")\n",
    "print(f\"✅ Tokenizer caricato correttamente su: {OUTPUT_REPO}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:50:26.486446Z",
     "iopub.status.busy": "2025-05-10T10:50:26.486167Z",
     "iopub.status.idle": "2025-05-10T10:50:27.868676Z",
     "shell.execute_reply": "2025-05-10T10:50:27.867800Z",
     "shell.execute_reply.started": "2025-05-10T10:50:26.486426Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SALVATAGGIO MODELLO LoRA + TOKENIZER\n",
    "print(\"💾 Salvataggio del modello e tokenizer...\")\n",
    "trainer.model.save_pretrained(SAVE_DIRECTORY)\n",
    "tokenizer.save_pretrained(SAVE_DIRECTORY)\n",
    "print(f\"✅ Modello LoRA salvato in: {SAVE_DIRECTORY}\")\n",
    "\n",
    "# SALVATAGGIO LOSS (è stato usato LossLoggerCallback)\n",
    "loss_path = os.path.join(SAVE_DIRECTORY, \"loss_log.csv\")\n",
    "\n",
    "if hasattr(trainer.callback_handler.callbacks[0], \"train_losses\"):\n",
    "    print(\"📊 Esportazione delle loss in CSV...\")\n",
    "    losses = trainer.callback_handler.callbacks[0].train_losses\n",
    "    loss_df = pd.DataFrame(losses, columns=[\"step\", \"loss\"])\n",
    "    loss_df.to_csv(loss_path, index=False)\n",
    "    print(f\"✅ Loss salvate in {loss_path}\")\n",
    "else:\n",
    "    print(\"⚠️ Nessuna loss trovata nei callback!\")\n",
    "\n",
    "\n",
    "# VISUALIZZAZIONE LOSS (solo se il file esiste)\n",
    "if os.path.exists(loss_path):\n",
    "    print(\"📈 Visualizzazione della training loss...\")\n",
    "    loss_df = pd.read_csv(loss_path)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.lineplot(data=loss_df, x=\"step\", y=\"loss\", marker=\"o\")\n",
    "\n",
    "    plt.title(\"Andamento della Training Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"❌ File {loss_path} non trovato. Salta visualizzazione.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizzazione della loss di addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-10T10:51:18.791507Z",
     "iopub.status.busy": "2025-05-10T10:51:18.791259Z",
     "iopub.status.idle": "2025-05-10T10:51:19.060204Z",
     "shell.execute_reply": "2025-05-10T10:51:19.059547Z",
     "shell.execute_reply.started": "2025-05-10T10:51:18.791491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def crate_loss_chart(json_file):\n",
    "    # Carica il file JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Estrai i dati di interesse\n",
    "    log_history = data.get(\"log_history\", [])\n",
    "    steps = [entry[\"step\"] for entry in log_history]\n",
    "    losses = [entry[\"loss\"] for entry in log_history]\n",
    "    \n",
    "    # Crea un DataFrame\n",
    "    df = pd.DataFrame({\"Step\": steps, \"Loss\": losses})\n",
    "    \n",
    "    # Crea il grafico a linee\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(data=df, x=\"Step\", y=\"Loss\", marker='o')\n",
    "    plt.title(\"Andamento della Loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    # Rotazione delle etichette e selezione dei ticks\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.locator_params(axis='x', nbins=10)  # Mostra solo 10 ticks sull'asse x\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()  # Migliora la disposizione degli elementi\n",
    "    plt.show()\n",
    "\n",
    "loss_path = \"/kaggle/working/trainer_output/checkpoint-1170/trainer_state.json\"\n",
    "crate_loss_chart(loss_path)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
